{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [a for a in np.arange(3)]\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "import numpy as np\n",
    "from scipy.linalg import lu_factor, lu_solve\n",
    "from scipy.linalg import qr, solve_triangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (985605389.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [10]\u001b[0;36m\u001b[0m\n\u001b[0;31m    np.gradient(enrollemt_diff, *varargs=1)\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "enrollemt = np.array([39896, 40434, 41294, 42023, 42355, 43035, 43859, 43617, 44157, 44939])\n",
    "\n",
    "enrollemt_diff = []\n",
    "\n",
    "for i in range(len(enrollemt)-1):\n",
    "    enrollemt_diff.append(enrollemt[i+1] - enrollemt[i])\n",
    "\n",
    "print('Enrollment change each year:',enrollemt_diff)\n",
    "\n",
    "np.gradient(enrollemt_diff, varargs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmat = np.array([[1,1,1,1,1], [0,1,2,3,4], [0,1,4,9,16],[0,1,8,27,64],[0,1,16,81,256]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1434, 0.0755, 0.0325],\n",
      "        [0.8175, 0.9428, 0.9265],\n",
      "        [0.5726, 0.4585, 0.3667],\n",
      "        [0.5515, 0.6069, 0.3442],\n",
      "        [0.1329, 0.6872, 0.4325]], requires_grad=True) tensor([[-0.3078, -0.0509, -0.5352, -0.1471, -0.8418, -0.2045, -0.0681, -0.7373,\n",
      "         -0.6520, -0.0130],\n",
      "        [ 0.8087,  0.7139,  1.1394,  1.4977, -0.1948,  0.5544,  1.8764,  1.0141,\n",
      "          0.6360,  1.1029],\n",
      "        [-0.0616,  0.2335, -0.1861,  0.3593, -0.4036,  0.3157,  0.8308, -0.1285,\n",
      "          0.6561,  0.5494],\n",
      "        [ 0.0816, -0.1369, -0.1324,  0.6796, -0.3780,  0.0200,  1.3905,  0.8364,\n",
      "          0.2325,  0.5091],\n",
      "        [ 0.2381,  0.0512,  0.4298,  0.2060, -0.1892,  0.2719,  0.8697,  0.2722,\n",
      "         -0.0128,  0.1052]], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1434, 0.0755, 0.0325],\n",
       "         [0.8175, 0.9428, 0.9265],\n",
       "         [0.5726, 0.4585, 0.3667],\n",
       "         [0.5515, 0.6069, 0.3442],\n",
       "         [0.1329, 0.6872, 0.4325]], requires_grad=True),\n",
       " tensor([[-1.7763, -3.9981, -3.8859],\n",
       "         [ 9.4915, 14.5071, 10.6797],\n",
       "         [ 3.1194,  3.6353,  2.7947],\n",
       "         [ 5.0529,  6.0803,  4.2629],\n",
       "         [ 2.7305,  3.6761,  3.3175]]),\n",
       " tensor([[0.5177, 0.1725, 0.0372, 0.7000, 0.0067, 0.1933, 0.9965, 0.3880, 0.2937,\n",
       "          0.5053],\n",
       "         [0.7663, 0.9732, 0.4779, 0.7982, 0.2555, 0.3200, 0.9644, 0.9804, 0.3980,\n",
       "          0.9199],\n",
       "         [0.4984, 0.1245, 0.9994, 0.2902, 0.1297, 0.2999, 0.9609, 0.5170, 0.8983,\n",
       "          0.2130]], requires_grad=True),\n",
       " tensor([[ 1.3167,  1.2826,  1.4644,  3.6223, -1.4895,  1.3036,  5.7646,  2.2942,\n",
       "           1.8573,  3.0181],\n",
       "         [ 1.8483,  1.4567,  2.3270,  4.2392, -1.5834,  1.7019,  7.1727,  3.0724,\n",
       "           1.9671,  3.3438],\n",
       "         [ 1.6955,  1.4409,  2.2207,  3.6751, -1.1356,  1.4945,  5.7912,  2.5481,\n",
       "           1.7663,  2.8871]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand((5,3), requires_grad=True)\n",
    "c = torch.rand((3,10), requires_grad=True)\n",
    "d = torch.rand((5,10))\n",
    "b = torch.matmul(a,c) - d\n",
    "print(a, b)\n",
    "torch.sum(torch.pow(b, 2)).backward()\n",
    "a, a.grad,c, c.grad\n",
    "\n",
    "# torch.sum(b),torch.mean(b)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "daf302ee44f1a774dac5ffd3a70ddee2099eff9a71d31c63aef06c6b735ae96f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 596 Summer HW3: NEWS RECOMMENDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We use MIcrosoft News Dataset (MIND)** is a large-scale dataset for news recommendation research. It was collected from anonymized behavior logs of Microsoft News website. The mission of MIND is to serve as a benchmark dataset for news recommendation and facilitate the research in news recommendation and recommender systems area.\n",
    "\n",
    "MIND contains about **160k English news** articles and **more than 15 million impression logs** generated by **1 million users**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Of The File :\n",
    "1. <a href='#1'> Importing The Packages </a>\n",
    "2. <a href='#2'> Importing The Data </a>\n",
    "3. <a href='#3'>Having a look at the data</a>\n",
    "4. <a href='#4'>Selecting Columns</a>\n",
    "5. <a href='#5'>Visualizing The data</a>  <ol>\n",
    "    <li> <a href='#5.1'>Category and Subcategory distribution in the data</a></li>\n",
    "    <li> <a href='#5.2'>WordClouds For Sports</a></li>\n",
    "    <li> <a href='#5.3'>Wordcloud For News</a></li>\n",
    "    <li> <a href='#5.4'>Title Length Distribution per Category</a></li></ol>\n",
    "\n",
    "6. <a href='#6'>Data Preprocessing</a> <ol>\n",
    "    <li> <a href='#6.1'>Checking and removing all the duplicate values</a></li>\n",
    "    <li> <a href='#6.2'>Checking for NaN values</a></li>\n",
    "    <li> <a href='#6.3'>Getting Titles with more than 4 words</a></li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# Importing The Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "# Below libraries are for feature representation using sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Below libraries are for similarity matrices using sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from re import sub\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "# import eli5\n",
    "# from eli5.lime import TextExplainer\n",
    "# from eli5.lime import TextExplainer\n",
    "# from eli5.lime.samplers import MaskingTextSampler\n",
    "# import seaborn as sns\n",
    "# from wordcloud import WordCloud\n",
    "# from xgboost import XGBClassifier\n",
    "plotly.offline.init_notebook_mode (connected = True)\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/qingchuanhou/Documents/GitHub/Learning-Note/Machine_Learning/EEP596_RecSys/Homeworks/HW3/EEP596HW3_QingchuanHou.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/qingchuanhou/Documents/GitHub/Learning-Note/Machine_Learning/EEP596_RecSys/Homeworks/HW3/EEP596HW3_QingchuanHou.ipynb#ch0000006?line=0'>1</a>\u001b[0m data\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mnews.tsv\u001b[39m\u001b[39m'\u001b[39m,header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('news.tsv',header=None,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns=['News ID',\n",
    "\"Category\",\n",
    "\"SubCategory\",\n",
    "\"Title\",\n",
    "\"Abstract\",\n",
    "\"URL\",\n",
    "\"Title Entities\",\n",
    "\"Abstract Entities \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "# Having a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "# Selecting the needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.iloc[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "# Visualizing The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.1'></a>\n",
    "# Category and Subcategory distribution in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=data[['Category','SubCategory']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=[]\n",
    "for i in c.index:\n",
    "    index.append(np.array(i))\n",
    "index=np.array(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['Category','Sub Category','Values'])\n",
    "df['Category']=index[:,0]\n",
    "df['Sub Category']=index[:,1]\n",
    "df['Values']=c.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(data_frame=df,x='Category',y='Values',color='Sub Category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From this we can clearly see that the most of the news was on the Category of news with subcategory newsus followed by sports with football news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.2'></a>\n",
    "# WordCloud For Sports News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=' '\n",
    "for i in data[data['Category']=='sports']['Title']:\n",
    "    text+=i+' '\n",
    "    \n",
    "# Make the figure\n",
    "wordcloud = WordCloud().generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.3'></a>\n",
    "# Wordcloud for news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=' '\n",
    "for i in data[data['Category']=='news']['Title']:\n",
    "    text+=i+' '\n",
    "    \n",
    "# Make the figure\n",
    "wordcloud = WordCloud().generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow the main attraction of news has been Trump :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5.4'></a>\n",
    "# Histogram For Title Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[]\n",
    "for i in data['Title']:\n",
    "    t.append(len(i))\n",
    "px.histogram(t,color=data['Category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well most of the titles are in the range of 50 to 100 words :) So if you are gonna write a news article keep the title in 50 - 100 words :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.1'></a>\n",
    "## Checking and removing all the duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the number of articles before processing :',len(data))\n",
    "data.drop_duplicates(subset=['Title'],inplace=True)\n",
    "print('The number of articles after processing :',len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.2'></a>\n",
    "## Checking for NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "sns.heatmap(data.isnull(), cbar=True, cmap='magma')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.3'></a>\n",
    "## Getting Titles with more than 4 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the number of articles before processing :',len(data))\n",
    "data=data[data['Title'].apply((lambda x: len(x.split())>=4))]\n",
    "print('The number of articles after processing :',len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "# Let's Make A Category Recommendation System\n",
    "use your X and Y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "daf302ee44f1a774dac5ffd3a70ddee2099eff9a71d31c63aef06c6b735ae96f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

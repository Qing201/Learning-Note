{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems PMP | Summer 2022 | Programming Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Objective:\n",
    "As we saw in the lecture, matrix factorization methods are able to address the limitations of SVD. In this assignment, we compare and contrast different approaches for the popular Movie Lens data set. Specifically, we'll understand the workings of MF techniques and compare their performance in the context of rating prediction problem in Recommender Systems. We will compare the performance of a un-personalized method (global average rating) with personalized methods like SVD and SGD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in and complete the code in the notebook below - You will be graded on code completeness AND results as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Dataset Description - Movie Lens 100K\n",
    "Movie Lens is a popular data set used to compare the performance of recommendation algorithms in academia. Movie Lens 100K is a subset of the complete data and it has the following characteristics:\n",
    "1. 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
    "1. Each user has rated at least 20 movies.\n",
    "1. The user item rating matrix is sparse. Only 100K out of the possible ~1.5 Million (943 * 1682) entries are present.\n",
    "\n",
    "More details can be found here - https://grouplens.org/datasets/movielens/100k/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset\n",
    "Since the dataset is sparse, we will utilize the csr_matrix format to store the data. CSR (Compressed Sparse Row) is a sparse matrix format which only stores the non-zero entries of the matrix. More details on this format and the python's scipy implementation can be found here:\n",
    "1. https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)\n",
    "1. https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html\n",
    "\n",
    "Apart from CSR, other popular sparse matrix formats are CSC (Compressed Sparse Column) and COO (Coordinate list or triplets). Details can be found here - https://en.wikipedia.org/wiki/Sparse_matrix\n",
    "\n",
    "For notational convention, we will use \n",
    "1. '_sparse' for sparse version of the matrix (for ex: train_sparse, test_sparse) \n",
    "1. '_dense' for dense version of the matrix (for ex: train_dense, test_dense) \n",
    "1. '_hat\\_()' for the predicted values (for ex: R_hat_svd for predicted rating matrix of svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_class\n",
    "\n",
    "Make sure you understand what each of X_*, Y_* and *_sparse data structures and the difference between them in this class\n",
    "1. X_* represents the non-zero indices (row, col) in the matrix and Y_* represents the corresponding non-zero values\n",
    "1. *_sparse represents the csr_matrix version of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "#from mxnet import nd\n",
    "#from mxnet import gluon\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "class data_class:\n",
    "    def __init__(self):\n",
    "        prefix = './ml-100k/' # This is where your data is stored - CHANGE it to an appropriate LOCAL drive\n",
    "        header = ['user', 'movie', 'rating', 'timestamp']\n",
    "        self.train_file = prefix + \"u1.base\"\n",
    "        self.test_file = prefix + \"u1.test\"\n",
    "        \n",
    "        self.X_train, self.Y_train, self.train_sparse = self.get_data_from_file(self.train_file, header)\n",
    "        self.mx_row = self.train_sparse.shape[0] \n",
    "        self.mx_col = self.train_sparse.shape[1] \n",
    "        self.X_test, self.Y_test, self.test_sparse = self.get_data_from_file(self.test_file, header)\n",
    "    \n",
    "    def get_data_from_file(self, file_name, header):\n",
    "        df = pd.read_csv(file_name, delimiter='\\t', header = None)\n",
    "        df.columns = header\n",
    "        print(\"Read: %s, Rows: %d, Cols: %d\" % (file_name, df.shape[0], df.shape[1]))\n",
    "        return self.get_sparse_matrix(df, header[0], header[1], header[2], 943, 1682)\n",
    "        \n",
    "    def get_sparse_matrix(self, df, row_name, col_name, data_name, n_rows, n_cols):\n",
    "        rows = np.array(df[row_name])\n",
    "        cols = np.array(df[col_name])\n",
    "        X = np.array(np.vstack((rows - 1, cols - 1))).T\n",
    "        data = np.array(df[data_name])\n",
    "        # start from index 0\n",
    "        sparse_mat = csr_matrix((data, (rows - 1, cols - 1)), shape = (n_rows, n_cols), dtype = float)\n",
    "        return X, np.array(data).reshape((data.shape[0], 1)), sparse_mat\n",
    "\n",
    "    def add_data_loader(self, data_loader):\n",
    "        self.data_loader = data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read: ./ml-100k/u1.base, Rows: 80000, Cols: 4\n",
      "Read: ./ml-100k/u1.test, Rows: 20000, Cols: 4\n",
      "(80000, 2)\n",
      "[[ 0  0]\n",
      " [ 0  1]\n",
      " [ 0  2]\n",
      " [ 0  3]\n",
      " [ 0  4]\n",
      " [ 0  6]\n",
      " [ 0  7]\n",
      " [ 0  8]\n",
      " [ 0 10]\n",
      " [ 0 12]\n",
      " [ 0 14]\n",
      " [ 0 15]\n",
      " [ 0 17]\n",
      " [ 0 18]\n",
      " [ 0 20]\n",
      " [ 0 21]\n",
      " [ 0 24]\n",
      " [ 0 25]\n",
      " [ 0 27]\n",
      " [ 0 28]\n",
      " [ 0 29]\n",
      " [ 0 31]\n",
      " [ 0 33]\n",
      " [ 0 34]\n",
      " [ 0 36]]\n"
     ]
    }
   ],
   "source": [
    "d_inst = data_class()\n",
    "print(d_inst.X_train.shape)\n",
    "print(d_inst.X_train[:25,:])\n",
    "#print(d_inst.Y_train[:25,:])\n",
    "#print(d_inst.X_test[:25,:])\n",
    "#print(d_inst.Y_test[:25,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Characteristics\n",
    "\n",
    "Before starting to build a model, it is a good idea to understand the characteristics of the data. Now, we'll compute the following simple statistics of this data set\n",
    "1. Size of the rating matrix \n",
    "1. Rating distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read: ./ml-100k/u1.base, Rows: 80000, Cols: 4\n",
      "Read: ./ml-100k/u1.test, Rows: 20000, Cols: 4\n",
      "Training Matrix\n",
      "[[5. 3. 4. 3. 3.]\n",
      " [4. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "(943, 1682)\n",
      "80000\n",
      "Test Matrix\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [4. 3. 0. 0. 0.]]\n",
      "(943, 1682)\n",
      "20000\n",
      "80000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the read matrices\n",
    "data_inst = data_class()\n",
    "\n",
    "print(\"Training Matrix\")\n",
    "print(data_inst.train_sparse[:5, :5].todense())\n",
    "print(data_inst.train_sparse.shape)\n",
    "print(data_inst.train_sparse.nnz)\n",
    "\n",
    "print(\"Test Matrix\")\n",
    "print(data_inst.test_sparse[:5, :5].todense())\n",
    "print(data_inst.test_sparse.shape)\n",
    "print(data_inst.test_sparse.nnz)\n",
    "\n",
    "# Distribution plots\n",
    "ratings = data_inst.Y_train\n",
    "print(len(ratings))\n",
    "plt.hist(ratings, bins = np.arange(0, 7), align = 'left', width = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metric\n",
    "To evaluate the performance of each method, we will utilize RMSE (Root Mean Squared Error) as the evaluation metric which computes the rating prediction error. Given 'r' as the actual rating, 'r_hat' the predicted rating and 'n' ratings to predict in test set, RMSE is defined as, sqrt(mean((y_hat - y)**2)) for all ratings in test set.\n",
    "\n",
    "More details on RMSE can be found here - https://en.wikipedia.org/wiki/Root-mean-square_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(test_mat, r_hat):\n",
    "    (rows, cols) = test_mat.nonzero()\n",
    "    rmse = 0\n",
    "    for i in np.arange(test_mat.nnz):\n",
    "        actual = test_mat[rows[i], cols[i]]\n",
    "        prediction = r_hat[rows[i], cols[i]]\n",
    "        error = (prediction - actual)\n",
    "        rmse += (error * error)\n",
    "    \n",
    "    rmse /= (1.0 * test_mat.nnz)\n",
    "    rmse = math.sqrt(rmse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Global Average Rating\n",
    "Implment an un-personalized recommendation algorithm where the global average rating for each item over all users who have rated that item in the training data set is used as the predicted rating. This means, for a given movie the same prediction value is used for all (un-rated) users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 1682)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1682)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_inst.train_sparse.todense().shape)\n",
    "\n",
    "dense_mat = data_inst.train_sparse.todense()\n",
    "\n",
    "dense_mat[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read: ./ml-100k/u1.base, Rows: 80000, Cols: 4\n",
      "Read: ./ml-100k/u1.test, Rows: 20000, Cols: 4\n",
      "Item Averages: [3.89295039 3.18095238 3.         ... 2.         3.         3.        ]\n",
      "(943, 1682)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nl/jzl2sdyd5g7gv2bk8lfg29tr0000gn/T/ipykernel_71466/62485680.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  item_means = np.array(item_sums) / np.array(item_counts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.0361\n"
     ]
    }
   ],
   "source": [
    "# Compute rating means for each item\n",
    "def get_item_avg(sparse_mat):\n",
    "    ## YOUR CODE HERE\n",
    "    item_sums = []\n",
    "    item_counts = []\n",
    "    dense_mat = sparse_mat.todense()\n",
    "    for i in range(dense_mat.shape[1]):\n",
    "        item_rates = dense_mat[:,i]\n",
    "        item_sums.append(np.sum(item_rates))\n",
    "        item_counts.append(np.count_nonzero(item_rates))\n",
    "    item_means = np.array(item_sums) / np.array(item_counts)\n",
    "\n",
    "    # Handle NaN due to divide by zero\n",
    "    item_means[np.isnan(item_means)] = 0\n",
    "    return item_means\n",
    "\n",
    "data_inst = data_class()\n",
    "train_sparse = data_inst.train_sparse\n",
    "test_sparse = data_inst.test_sparse\n",
    "\n",
    "# Compute the average item rating \n",
    "item_means = get_item_avg(train_sparse)\n",
    "print(\"Item Averages: %s\" % (item_means))\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Compute the R_hat matrix.\n",
    "R_hat_avg_rating = np.tile(item_means, (test_sparse.shape[0], 1))\n",
    "print(R_hat_avg_rating.shape)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Compute test rmse\n",
    "test_rmse = rmse(test_sparse, R_hat_avg_rating)\n",
    "print(\"Test RMSE: %.4f\" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global item average rating seems to be doing pretty well with a RMSE close to 1.0. Any guesses on why do you think it is a doing a good job even though it is a non-personalized method?\n",
    "Hint: Plot and compare the item average rating histogram with test set's rating histogram.\n",
    "\n",
    "(Write down your answer here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the item_means distribution\n",
    "plt.hist(item_means, bins = np.arange(0, 7), align = 'left', width = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Sparse SVD\n",
    "Perform SVD on the matrix train_sparse to compute the 'k' dimensional user and item factor matrices. Utilize the user and item factor matrices to predict the ratings for test_sparse matrix and then compute the RMSE.\n",
    "\n",
    "Note, \n",
    "1. To compute the SVD, fill in the missing entries in train_sparse with 0. \n",
    "1. Since train_sparse is a sparse matrix, utilize the svds method from scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse SVD\n",
    "import scipy.sparse.linalg as sla\n",
    "\n",
    "data_inst = data_class()\n",
    "train_sparse = data_inst.train_sparse\n",
    "n_factors = 10\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Compute the sparse svd using svds\n",
    "Uslice, Sslice, Vslice = \n",
    "print Uslice.shape, Sslice.shape, Vslice.shape\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Compute the R_hat_svd\n",
    "R_hat_svd = \n",
    "print(\"Training Loss: %.15f\" % (la.norm(train_sparse.todense() - R_hat_svd)))\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Compute the RMSE for test set using the rmse() function\n",
    "test_rmse = \n",
    "print(\"Test RMSE: %.4f\" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD seems to be doing a poor job compared to Global Item Average Rating in terms of RMSE on test set. Any guesses on why this is so? This about this before heading into the next question.\n",
    "Hint: What are we filling the missing values in the matrix with? How does that value affect the SVD factorization?\n",
    "\n",
    "We'll explore more on this in the next problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: SVD with Imputed Values\n",
    "The not so good performane of the SVD in terms of RMSE on test set can be attributed to the fact that we are imputing the missing values with zeros. Thus, SVD tries to factorize the imputed matrix to fit zeros for missing values, thereby increasing the test RMSE. Instead of imputing the missing values with zeros, we can do a better job by utilizing the existing training data and impute the missing value with either the global item average rating or user average rating. Utilize the item_means that you calculated before to fill in the missing values in the train array before computing the SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute with Item/User Averages\n",
    "train_dense = train_sparse.todense()\n",
    "n_factors = 5\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Compute item averages (utilize the code that you implemented in problem 1)\n",
    "item_means = \n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Replace zeros in train_dense with their corresponding item average value. \n",
    "# You can get/set a specific value (i, j) in train_dense using train_dense[i, j]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Now that the matrix is dense, utilize the dense svd to compute the factor matrices\n",
    "U, S, V = \n",
    "print U.shape, S.shape, V.shape\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Compute the sliced matrices based on n_factors\n",
    "Uslice = \n",
    "Sslice = \n",
    "Vslice = \n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Compute R_hat_dense_svd\n",
    "R_hat_dense_svd = \n",
    "print(\"Training Loss: %.15f\" % (la.norm(train_dense - R_hat_dense_svd)))\n",
    "\n",
    "# Compute test RMSE\n",
    "test_rmse = rmse(test_sparse, R_hat_dense_svd)\n",
    "print(\"Test RMSE: %.4f\" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by imputing with item averages the performance drastically improved and it now performs better than item global average (non-personalized method). How would the performance change if user average ratings are utilized instead of item average ratings? Go back and try it out and see which one performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: SVD on Global Average Rating Values \n",
    "In problem 3, we imputed the missing values of user rating matrix with global item average ratings and performed the SVD on the imputed matrix. Instead, what if we utilized only the global average ratings for each user adn then performed SVD on that. That is, every user has the same set of ratings (same as R_hat_avg_rating from Problem 1). How would the performance (test RMSE) change based on changing n_factors (20, 10, 5, 1)? Do you observe anything strange? \n",
    "\n",
    "Hint: Examine the singular values in S matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = 5\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Utilize the R_hat_avg_rating to compute the SVD factor matrices\n",
    "U, S, V = \n",
    "print U.shape, S.shape, V.shape\n",
    "\n",
    "Uslice = U[:, :n_factors]\n",
    "Sslice = S[:n_factors]\n",
    "Vslice = V[:n_factors, :]\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Compute the R_hat_svd_avg_rating matrix using the slice matrices from above\n",
    "R_hat_svd_avg_rating = \n",
    "print(\"Training Loss: %.15f\" % (la.norm(train_dense - R_hat_svd_avg_rating)))\n",
    "\n",
    "test_rmse = rmse(test_sparse, R_hat_svd_avg_rating)\n",
    "print(\"Test RMSE: %.4f\" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: SGD\n",
    "Implement SGD method to compute the user and item factor matrices (U and V). (In the assignment code below, we use Pytorch (actually Torch) as the framework for implementing automatic differentiation. However, you can also use Pytorch to do the same.)  Utilize the computed U and V matrices to compute the prediction for test_sparse matrix and compute the RMSE. Tune different hyperparameters and understand how they impact the train and test RMSE. \n",
    "\n",
    "This problem is broken down into multiple classes. You need to fill in some code in each of the classes. Each class has an associated unit test function which will help you validate if you have coded that class correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Model Class \n",
    "\n",
    "This class contain the model parameters and computes the rating predictions based on the current state of the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class model:\n",
    "    def __init__(self, n_factors, mx_row, mx_col):\n",
    "        self.n_factors = n_factors\n",
    "        self.mx_row = mx_row\n",
    "        self.mx_col = mx_col\n",
    "        self.initialize_model_parameters()\n",
    "    \n",
    "    def initialize_model_parameters(self):\n",
    "        ## YOUR CODE HERE\n",
    "        # Initialize the model parameters U and V using torch random matrix initializer\n",
    "        # Make sure to require gradient support (requires_grad) parameter\n",
    "        self.U = \n",
    "        self.V = \n",
    "        \n",
    "    \n",
    "    def simple_prediction(self, u, v):\n",
    "        ## YOUR CODE HERE\n",
    "        # Compute the prediction for a given user 'u' and item 'v' vector\n",
    "        # Refer back to lecture on how the prediction was calculated\n",
    "        predictions = \n",
    "        return predictions\n",
    "\n",
    "    def get_n_factors(self):\n",
    "        return self.n_factors    \n",
    "\n",
    "def test_model():\n",
    "    k = 10\n",
    "    m = 100\n",
    "    n = 200\n",
    "    model_inst = model(k, m, n)\n",
    "    print(\"U Shape: (%d, %d), Expected U shape: (%d, %d)\" % (model_inst.U.shape[0], model_inst.U.shape[1], m, k))\n",
    "    print(\"V Shape: (%d, %d), Expected V shape: (%d, %d)\" % (model_inst.V.shape[0], model_inst.V.shape[1], k, n))\n",
    "\n",
    "    \n",
    "test_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD - Metrics Class\n",
    "\n",
    "This class is a more sophisticated version of the RMSE function we saw earlier. Along with computing the RMSE for test set, this class also contains computation of loss function for the SGD iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class metrics:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def square_loss(self, y_hat, y):\n",
    "        ## YOUR CODE HERE\n",
    "        # Return the normalized squared loss between y_hat and y\n",
    "        return \n",
    "    \n",
    "    def RMSE(self, y_hat, y):\n",
    "        return torch.pow(self.square_loss(y_hat,y),0.5)\n",
    "    \n",
    "    def all_loss_RMSE(self, X, Y, n_factors, model_inst):\n",
    "        \n",
    "        # Compute loss and RMSE and return both\n",
    "        with torch.no_grad():\n",
    "            pred = torch.matmul(model_inst.U,model_inst.V)\n",
    "        return self.square_loss(pred, torch.tensor(Y).reshape(-1)),  self.RMSE(pred, torch.tensor(Y).reshape(-1))\n",
    "\n",
    "\n",
    "def test_metrics():\n",
    "    x = torch.tensor([1, 2, 3, 4])\n",
    "    x = x.type(torch.FloatTensor)\n",
    "    y = torch.tensor([4, 5, 6, 7])\n",
    "    y = y.type(torch.FloatTensor)\n",
    "    expected_loss = 9.0\n",
    "    metrics_inst = metrics()\n",
    "    print(\"Square Loss: %f, Expected Square Loss: %f\" % (metrics_inst.square_loss(x, y), expected_loss))\n",
    "    \n",
    "test_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD - Optimizer Class\n",
    "\n",
    "This class updates the model parameters based on the computed gradients. This utilizes the autograd to compute the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizer:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def SGD(self, model_inst):\n",
    "        #for data in self.data_batch:\n",
    "        #print(\"Data shape = \", self.data_batch.shape)\n",
    "        indU = list(set(self.data_batch[:,0]))\n",
    "        indV = list(set(self.data_batch[:,1]))\n",
    "\n",
    "        ## YOUR CODE HERE\n",
    "        # Update the corresponding rows of U and V using the gradient update rule\n",
    "        # parameter = parameter - learning_rate * gradient(parameter)\n",
    "        # Use autograd to compute the gradients\n",
    "        with torch.no_grad(): # This is required to prevent gradient attached error\n",
    "            model_inst.U[indU,:] -=  # Formula for gradient descent for U - Key thing is to only update the relevant parts of U matrix\n",
    "            model_inst.V[:,indV] -=  # Formula for gradient descent for V - Key thing is to only update the relevant parts of V matrix\n",
    "\n",
    "    def optimization(self, data_batch, model_instance):\n",
    "        self.data_batch = data_batch\n",
    "        self.SGD(model_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD - Trainer Class\n",
    "\n",
    "This class performs the training iterations. It iterates through the training data samples and updates the model parameters based on their gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, learning_rate = 0.5, epochs = 10, regularization_weight = 0):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization_weight = regularization_weight\n",
    "        self.training_RMSE = np.zeros((self.epochs, 1))\n",
    "        self.testing_RMSE = np.zeros((self.epochs, 1))\n",
    "    \n",
    "    def print_metrics(self,epoch,train_rmse,test_rmse):\n",
    "        print(\"EPOCH = %d, train_rmse = %f, test_rmse = %f \\n\" % (epoch, float(train_rmse), float(test_rmse)))\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        return self.training_RMSE, self.testing_RMSE\n",
    "    \n",
    "    def train(self, model_instance, data_inst):\n",
    "        # Refresh model parameters\n",
    "        model_instance.initialize_model_parameters()\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        optimizer_inst = optimizer(self.learning_rate)\n",
    "        \n",
    "        # Initialize metrics\n",
    "        metrics_inst = metrics()\n",
    "        \n",
    "        print(data_inst.data_loader)\n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            count = 0\n",
    "            # Iterate over the mini batches\n",
    "            # Make sure you understand the shape of data and label before proceeding further\n",
    "            for data in data_inst.data_loader:\n",
    "                #print(data.shape)\n",
    "                #print(data)\n",
    "                ## YOUR CODE HERE\n",
    "                # Get the set of indices for U and V for the current batch of training samples in data\n",
    "                indU = data[:,0]\n",
    "                indV = data[:,1]\n",
    "                Y = data[:,2]\n",
    "                \n",
    "                ## YOUR CODE HERE\n",
    "                # Subset the rows of U and V to only indU and indV. Hint: use slicing operations\n",
    "                u = \n",
    "                v = \n",
    "\n",
    "                \n",
    "                ## YOUR CODE HERE\n",
    "                # Compute the prediction for the user 'u' and item 'v' using model_instance \n",
    "                pred = \n",
    "            \n",
    "                ## YOUR CODE HERE\n",
    "                # Compute the squared loss using metrics_inst\n",
    "                # The loss should only depend on the data that is part of the mini batch and \n",
    "                # also only on the subset of the parameters U,V that were computed couple of steps earlier. \n",
    "                loss = \n",
    "                        \n",
    "                \n",
    "                # Back Propagate gradients\n",
    "                loss.backward() # Once loss is computed, loss.backward() updates the gradient of the parameters and stores it as such\n",
    "                                # So once this is done, as an example, U.grad gives you the updated gradient for U, etc\n",
    "                \n",
    "                ## YOUR CODE HERE\n",
    "                # Optimization - update the model parameters using optimizer_inst\n",
    "                optimizer_inst.optimization(data,model_inst)\n",
    "                \n",
    "                count += 1\n",
    "                \n",
    "            # Compute the training and test loss and RMSE after each epoch!\n",
    "            training_loss, train_RMSE = metrics_inst.all_loss_RMSE(data_inst.X_train,\\\n",
    "                    data_inst.Y_train, model_instance.n_factors, model_instance)\n",
    "            test_loss, test_RMSE = metrics_inst.all_loss_RMSE(data_inst.X_test, \\\n",
    "                    data_inst.Y_test, model_instance.n_factors, model_instance)\n",
    "            \n",
    "            # Print metrics\n",
    "            self.print_metrics(epoch, train_RMSE, test_RMSE)\n",
    "            \n",
    "            self.training_RMSE[epoch, 0] = train_RMSE\n",
    "            self.testing_RMSE[epoch, 0] = test_RMSE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD - Config Class\n",
    "\n",
    "This class contains the set of hyper parameters that the SGD algorithm uses. Tuning these parameters is critical to get the best performance out of the algorithm.\n",
    "For this specific data set, we have provided a range of values for each of these hyper parameters to test. To start, pick a value for each hyper parameter in the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    def __init__(self):\n",
    "        self.config()\n",
    "        \n",
    "    def config(self):\n",
    "        ## YOUR CODE HERE\n",
    "        # Initialize the model hyper parameters based on the suggested range\n",
    "        self.batch_size = 64 # (50, 200)\n",
    "        self.learning_rate =   # (1e-3, 0.1, 1.0)\n",
    "        self.epochs =  10 # (5, 10, 100) # 10 might be a good point to start, increasing this increases the optimization run time linearly\n",
    "        self.n_factors =   # (5, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Visualizing Errors\n",
    "\n",
    "To visualize the train and test errors as a function of number of iterations of the SGD algorithm, the following code snippet provides the functionality to generate the plot. Collect the train and test errors in each iteration and call this method to generate the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_metrics(train_RMSE, test_RMSE):\n",
    "    epochs = range(len(train_RMSE))\n",
    "    plt.plot(epochs, train_RMSE, 'b', label = \"Train RMSE\")\n",
    "    plt.plot(epochs, test_RMSE, 'g', label = \"Test RMSE\")\n",
    "    plt.title('RMSE vs Epochs')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD - Main function\n",
    "\n",
    "This is the main function which binds all the functionality we have created so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SGD Main Function\n",
    "# Load configuration of hyper parameters\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "config_inst = config()\n",
    "\n",
    "# Load train and test data\n",
    "data_inst = data_class()\n",
    "\n",
    "# Data Loader for training data\n",
    "# This is helpful to create mini-batches for optimization\n",
    "XY_train = torch.tensor(np.hstack((data_inst.X_train,data_inst.Y_train)))\n",
    "data_loader = DataLoader(XY_train,batch_size=config_inst.batch_size,shuffle=True) \n",
    "# shuffle=True ensures there is random shuffling of batches when sampling from DataLoader - mimicking random mini-batches\n",
    "\n",
    "data_inst.add_data_loader(data_loader)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Create a model instance based on the corresponding config_inst values\n",
    "rows = # Plug in the appropriate number of rows\n",
    "cols = # Plug in the appropriate number of cols\n",
    "\n",
    "model_inst = model(config_inst.n_factors,rows,cols)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Create a Trainer Instance based on the corresponding config_inst values\n",
    "trainer_inst = \n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Train the model using the trainer instance, model_inst and data_inst\n",
    "trainer_inst.train(model_inst,data_inst)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "# Get the training and test metrics using trainer_inst\n",
    "training_RMSE, testing_RMSE = \n",
    "\n",
    "# Plot the Errors\n",
    "plot_metrics(training_RMSE, testing_RMSE) # If you go the SGD mini-batch going right, this plot will curve downwards for both train and test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though SGD is performing slightly better than user/item average imputed SVD, we expect the SGD to perform much better. Looking at the train and test error plot, can you guess what might be the issue with the current optimization? And what can we do to fix it?\n",
    "\n",
    "Hint: Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6: SGD with Regularization\n",
    "\n",
    "From the error plot in Problem 5 we can see that the test RMSE decreases with train RMSE for first few iterations, but after that the test RMSE starts to increase or remains flat, even though the train RMSE is increasing. This is a good indicator for overfitting. Like we say in the lecture, adding regularization is one of the ways to fix this. \n",
    "\n",
    "Implement the l2 regularization to the SGD code above and compare the performence with and without the regularization. Do you see the expected benefit?\n",
    "\n",
    "Hint: Following are the specific changes you need to do in the code to add the regularization\n",
    "\n",
    "1. Update the config class to include a regularization weight hyper parameter (regularization_weight). A value in (0.1, 10) is a good range to try for this hyper parameter.\n",
    "1. Add a function in metrics class to compure the square loss with regularization. Signature of that function is given by square_loss_regularized(self, y_hat, y, u, v, regularization_weight)\n",
    "1. Update the trainer class constructor to take in the regularization_weight as a parameter\n",
    "1. Update the Trainer.train() method to compute the new loss with regualrization. Remember, autograd will take care of computing the updated gradient, as long as the loss function computation is correct.\n",
    "1. Finally update the main() to pass in the regularization_weight parameter to the trainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "daf302ee44f1a774dac5ffd3a70ddee2099eff9a71d31c63aef06c6b735ae96f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

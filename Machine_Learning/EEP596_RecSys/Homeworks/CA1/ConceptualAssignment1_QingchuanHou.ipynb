{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEP 596 Conceptual Assignment 1\n",
    "### Qingchuan Hou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Hand Computation\n",
    "$\n",
    "X = \\begin{bmatrix}\n",
    "4 & -3 \\\\\n",
    "-2 & 5 \n",
    "\\end{bmatrix} \n",
    "$\n",
    "\n",
    "**(1)** $\n",
    "X_T = \\begin{bmatrix}\n",
    "4 & -2 \\\\\n",
    "-3 & 5 \n",
    "\\end{bmatrix} \n",
    "$\n",
    "\n",
    "**(2)** $Tr(X) = x_{11} + x_{22} = 4 + 5 = 9$\n",
    "\n",
    "\n",
    "**(3)** $||X||_F$\n",
    "$$\n",
    "\\begin{align*}\n",
    "||X||_F & = \\sqrt{Tr(X^T X)} \\\\\n",
    "        & = \\sqrt{Tr(\\begin{bmatrix}\n",
    "                    4 & -2 \\\\\n",
    "                    -3 & 5 \n",
    "                    \\end{bmatrix}  \n",
    "                    \\begin{bmatrix}\n",
    "                    4 & -3 \\\\\n",
    "                    -2 & 5 \n",
    "                    \\end{bmatrix} )} \\\\\n",
    "        & = \\sqrt{Tr(\\begin{bmatrix}\n",
    "                    4^2+(-2)^2 & 4*(-3)+5*(-2) \\\\\n",
    "                    -3*4+5*(-2) & (-3)^2 + 5^2\n",
    "                    \\end{bmatrix} )} \\\\\n",
    "        & = \\sqrt{Tr(\\begin{bmatrix}\n",
    "                    20 & -22 \\\\\n",
    "                    -22 & 34 \n",
    "                    \\end{bmatrix} )} \\\\\n",
    "        & = \\sqrt{20+34} \\\\\n",
    "        & = \\sqrt{54} \\\\\n",
    "        & \\approx 7.3\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**(4)** $2 \\times X = \n",
    "\\begin{bmatrix}\n",
    "2*4 & 2*(-3) \\\\\n",
    "2*(-2) & 2*5 \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "8 & -6 \\\\\n",
    "-4 & 10 \n",
    "\\end{bmatrix} $\n",
    "\n",
    "**(5)** \n",
    "$ X + I = \\begin{bmatrix}\n",
    "4 & -3 \\\\\n",
    "-2 & 5 \n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "5 & -3 \\\\\n",
    "-2 & 6 \n",
    "\\end{bmatrix} \n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Python Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[4, -3], [-2, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.T: [[ 4 -2]\n",
      " [-3  5]]\n",
      "\n",
      "Trace of X: 9\n",
      "\n",
      "Frobenius Norm of X: 7.3484692283495345\n",
      "\n",
      "2 x X: [[ 8 -6]\n",
      " [-4 10]]\n",
      "\n",
      "X + I: [[ 5. -3.]\n",
      " [-2.  6.]]\n"
     ]
    }
   ],
   "source": [
    "print('X.T:',X.T)\n",
    "print('\\nTrace of X:', X.trace())\n",
    "print('\\nFrobenius Norm of X:', np.linalg.norm(X))\n",
    "print('\\n2 x X:', 2*X)\n",
    "\n",
    "I = np.eye(X.shape[0], X.shape[1])\n",
    "print('\\nX + I:', X+I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Hand Compute\n",
    "$ X - I = \\begin{bmatrix}\n",
    "4 & -3 \\\\\n",
    "-2 & 5 \n",
    "\\end{bmatrix} -\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "3 & -3 \\\\\n",
    "-2 & 4 \n",
    "\\end{bmatrix}, \\ \\ \\ \\ \n",
    "(X-I)^T = \\begin{bmatrix}\n",
    "3 & -2 \\\\\n",
    "-3 & 4 \n",
    "\\end{bmatrix}, \n",
    "$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "||X-I||_F & = \\sqrt{Tr((X-I)^T (X-I))} \\\\\n",
    "        & = \\sqrt{Tr(\\begin{bmatrix}\n",
    "                    3 & -2 \\\\\n",
    "                    -3 & 4 \n",
    "                    \\end{bmatrix}  \n",
    "                    \\begin{bmatrix}\n",
    "                    3 & -3 \\\\\n",
    "                    -2 & 4 \n",
    "                    \\end{bmatrix} )} \\\\\n",
    "        & = \\sqrt{Tr(\\begin{bmatrix}\n",
    "                    3^2+(-2)^2 & 3*(-3)+4*(-2) \\\\\n",
    "                    -3*3+4*(-2) & (-3)^2 + 4^2\n",
    "                    \\end{bmatrix} )} \\\\\n",
    "        & = \\sqrt{Tr(\\begin{bmatrix}\n",
    "                    13 & -17 \\\\\n",
    "                    -17 & 25 \n",
    "                    \\end{bmatrix} )} \\\\\n",
    "        & = \\sqrt{13+25} \\\\\n",
    "        & = \\sqrt{38} \\\\\n",
    "        & \\approx 6.2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Python Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius Norm of X-I: 6.164414002968976\n"
     ]
    }
   ],
   "source": [
    "Y = X - I\n",
    "print('Frobenius Norm of X-I:', np.linalg.norm(X-I))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 13., -17.],\n",
       "       [-17.,  25.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.T.dot(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.41222084, -0.12359436, -0.90266185],\n",
       "        [ 0.41537654, -0.90729099, -0.06546285],\n",
       "        [-0.81088613, -0.4019297 ,  0.42534245]]),\n",
       " array([12.30911408,  2.88624728,  0.39406485]),\n",
       " array([[-0.72846229,  0.66445578,  0.16685685],\n",
       "        [-0.65664265, -0.60774199, -0.44662075],\n",
       "        [-0.19535382, -0.4349117 ,  0.87902713]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.array([[4, -3, -1], [-2, 5, 2], [8, -6, -1]])\n",
    "\n",
    "# SVD of Z\n",
    "u, s, vh = np.linalg.svd(Z, full_matrices=False)\n",
    "u, s, vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.93051122, -3.15470127, -0.68732362],\n",
       "       [-2.00503947,  4.98878075,  2.02267592],\n",
       "       [ 8.03274374, -5.92710336, -1.14733594]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank-2 approximation of Z\n",
    "u_2 = u[:,:2]\n",
    "s_2 = np.diag(s[:2])\n",
    "vh_2 = vh[:2,:]\n",
    "\n",
    "Z_2 = u_2.dot(s_2).dot(vh_2)\n",
    "Z_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The relative error: 0.031153562088046625\n"
     ]
    }
   ],
   "source": [
    "error = np.linalg.norm(Z - Z_2) / np.linalg.norm(Z)\n",
    "print(\"The relative error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnitude of the error make sense. The matrix Z_2 is the rank-2 approximation of Z by using the truncated SVD. It reduce the size but keeps the main information. Therefore, the matrix Z_2 should be similar to the matrix Z. That's why the relative error is small. \n",
    "\n",
    "The truncated SVD is the sparse mode of full SVD, it keeps the primary information of the data matrix and reduces the size of the matrix. In machine learning, the data matrix is always huge and will cost a lot of memory and computation to calculate the matrix. Because the truncated SVD can be much smaller than the full SVD, it will reduce the use of memory and computation costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 - 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. B\n",
    "5. D\n",
    "6. A, B, C, D\n",
    "7. C\n",
    "8. D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "daf302ee44f1a774dac5ffd3a70ddee2099eff9a71d31c63aef06c6b735ae96f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

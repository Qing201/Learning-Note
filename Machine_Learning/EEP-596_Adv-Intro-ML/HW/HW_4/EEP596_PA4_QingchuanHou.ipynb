{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this short programming assignment we will look into applications of word embeddings in similarity search and nearest neighbors. We will also look at creating a video based on tSNE embeddings of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -1. Create a video from images\n",
    "Download any 1000 or more 'appropriate' and publicly available images from the web. This could be part of a data set or something specific that you picked up or are interested in.\n",
    "\n",
    "We discussed using tSNE to find image embeddings for these images. Apply the tSNE library to these images and construct low-dimensional embeddings for the images. Use these embeddings to then:\n",
    "\n",
    "a) Start at any random image in the data set \n",
    "\n",
    "b) Sequentially chain the next image to the previous image using a scoring function/probability based on the tSNE embedding. So you want to chain the most similar image to the current one and so on. Choose a frame rate that is appropriate to convert this chain of images into a video. Your video shouldn't be more than 3 minutes long.\n",
    "\n",
    "c) Upload this video to youtube and share a link with your submission. \n",
    "\n",
    "d) Feel free to share your video to Discord to see what cool videos we come up with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_path = \"/Users/qingchuanhou/Desktop/Samples/flower_train\"\n",
    "n = 1000\n",
    "\n",
    "def Img_list(path, width=224, height=224):\n",
    "    X = list()\n",
    "\n",
    "    for label in os.listdir(path):\n",
    "        if not label.startswith('.'):\n",
    "            label_path = os.path.join(path, label)\n",
    "\n",
    "            i = 0\n",
    "            for image_name in os.listdir(label_path):\n",
    "                image_path = os.path.join(label_path, image_name)\n",
    "                image = cv2.imread(image_path)\n",
    "                image_data = Img2data(image, width, height)\n",
    "                X += [image_data]\n",
    "                i += 1\n",
    "                if i > n/5 - 1:\n",
    "                    break\n",
    "    return np.array(X)      \n",
    "    \n",
    "\n",
    "\n",
    "def Img2data(image, width=224, height=224):\n",
    "    image_resize = cv2.resize(image, (width, height))\n",
    "    image_data = np.reshape(image_resize, width*height*3)\n",
    "    return image_data\n",
    "\n",
    "def Data2img(image_data, width=224, height=224):\n",
    "    image = np.reshape(image_data, (width,height,3))\n",
    "    return image\n",
    "\n",
    "flowr_list = Img_list(flow_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE in 1d\n",
    "def tSNE_sort1d(img_list):\n",
    "    tsne_data = TSNE(n_components=1, init='random').fit_transform(img_list)\n",
    "    image_sort = [x for _,x in sorted(zip(tsne_data, img_list))]\n",
    "    return image_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_out(images, width=224, height=224):\n",
    "    out = cv2.VideoWriter('tsne_1d.avi',cv2.VideoWriter_fourcc(*'DIVX'), 20, (width, height))\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        out.write(Data2img(images[i], width, height))\n",
    "\n",
    "    out.release()\n",
    "\n",
    "def tSNE_1dvideo(img_list, width=224, height=224):\n",
    "    image_sort = tSNE_sort1d(img_list)\n",
    "    video_out(image_sort, width, height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tSNE_1dvideo(flowr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tSNE in 2d\n",
    "def tSNE_2dvideo(img_list, width=224, height=224):\n",
    "    tsne_data2d = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(img_list)\n",
    "    out = cv2.VideoWriter('tsne_2d.avi',cv2.VideoWriter_fourcc(*'DIVX'), 20, (width, height))\n",
    "\n",
    "    a = int(np.random.randint(len(tsne_data2d)))\n",
    "    x = tsne_data2d[a][0]\n",
    "    y = tsne_data2d[a][1]\n",
    "    tsne_data2d = np.delete(tsne_data2d, a, 0)\n",
    "    out.write(Data2img(img_list[a], width, height))\n",
    "    img_list = np.delete(img_list, a, 0)\n",
    "    for a in range(len(img_list)):\n",
    "        min_dis = ((x - tsne_data2d[0][0])**2 + (y - tsne_data2d[0][1])**2)**0.5\n",
    "        m = 0\n",
    "        for i in range(len(tsne_data2d)):\n",
    "            if min(min_dis, ((x - tsne_data2d[i][0])**2 + (y - tsne_data2d[i][1])**2)**0.5) < min_dis:\n",
    "                min_dis = ((x - tsne_data2d[i][0])**2 + (y - tsne_data2d[i][1])**2)**0.5\n",
    "                m = i\n",
    "        x = tsne_data2d[m][0]\n",
    "        y = tsne_data2d[m][1]\n",
    "        tsne_data2d = np.delete(tsne_data2d, m, 0)\n",
    "        out.write(Data2img(img_list[m], width, height))\n",
    "        img_list = np.delete(img_list, m, 0)\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tSNE_2dvideo(flowr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide video frame and use tSNE sort\n",
    "video_path = '/Users/qingchuanhou/Desktop/Samples/170724_15_Setangibeach.mp4'\n",
    "\n",
    "def video_read(path, width=224, height=224):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    X = list()\n",
    "    while cap.isOpened():\n",
    "        try:\n",
    "            ret,frame = cap.read()\n",
    "            if ret == True:\n",
    "                image_resize = cv2.resize(frame, (width, height), fx=0,fy=0, interpolation = cv2.INTER_CUBIC)\n",
    "                image_data = np.reshape(image_resize, width*height*3)\n",
    "                X += [image_data]\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_img_list = video_read(video_path, width=480, height=480)\n",
    "# video_img_list = np.random.shuffle(video_img_list)\n",
    "tSNE_2dvideo(video_img_list, width=480, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort flower images using 1d tSNE: https://youtu.be/qEiuoSa4KfU\n",
    "\n",
    "Sort flower images using 2d tSNE: https://youtu.be/EAUB0SLwrzQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving into Cheat Sheet of Pandas Data Frame\n",
    "There are some useful functions for solving problems below when it comes to index and slice data frames. Let's go over them.\n",
    "More materials can be found here: https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf\n",
    "1. DataFrame() Construct a dataframe. Use it for putting a numpy ndarray into a dataframe.\n",
    "1. DataFrame.loc() Purely label-location based indexer for selection by label. Use it for selecting word vectors in the dataframe.     \n",
    "1. DataFrame.dot() Matrix multiplication with DataFrame. Use it for dot product of word vectors.\n",
    "1. DataFrame.sort_values() Sort by the values along either axis. Use it for sorting distance short to long.\n",
    "\n",
    "Below are examples of using these functions. You don't have to code anything in this block, just focus on understanding the functions and how it works in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define sample word vectors\n",
      "[[0.1 0.3 0.4 0.5]\n",
      " [0.3 0.4 0.9 0.5]\n",
      " [0.2 0.8 0.7 0.5]]\n",
      "\n",
      "Pandas Data Frame for word vectors\n",
      "         0    1    2    3\n",
      "word1  0.1  0.3  0.4  0.5\n",
      "word2  0.3  0.4  0.9  0.5\n",
      "word3  0.2  0.8  0.7  0.5\n",
      "\n",
      "Find the row corresponding to word1\n",
      "0    0.1\n",
      "1    0.3\n",
      "2    0.4\n",
      "3    0.5\n",
      "Name: word1, dtype: float64\n",
      "\n",
      "Calculate the dot product between word1 and word2\n",
      "0.76\n",
      "\n",
      "Calculate the dot product between word1 and rest of the words\n",
      "word2    0.76\n",
      "word3    0.79\n",
      "dtype: float64\n",
      "\n",
      "Sorted dot product values\n",
      "word3    0.79\n",
      "word2    0.76\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define a ndarray\n",
    "d = np.array([[0.1,0.3,0.4,0.5],[0.3,0.4,0.9,0.5],[0.2,0.8,0.7,0.5]], dtype=float, order='F')\n",
    "print(\"Define sample word vectors\")\n",
    "print(d)\n",
    "#Construct a dataframe from ndarray and index each row as word vectors\n",
    "df = pd.DataFrame(d,index = ['word1','word2','word3'])\n",
    "print(\"\\nPandas Data Frame for word vectors\")\n",
    "print(df)\n",
    "#Select word vector1 by its label\n",
    "print(\"\\nFind the row corresponding to word1\")\n",
    "print(df.loc['word1'])\n",
    "#Calculate dot product of word vector1 and word vector2\n",
    "print(\"\\nCalculate the dot product between word1 and word2\")\n",
    "dot_product = df.loc['word2'].dot(df.loc['word1'])\n",
    "print(dot_product)\n",
    "#Calculate dot product of word vector1 to the rest of words\n",
    "print(\"\\nCalculate the dot product between word1 and rest of the words\")\n",
    "words_rest = ['word2','word3']\n",
    "dot_product2 = df.loc[words_rest].dot(df.loc['word1'])\n",
    "print(dot_product2)\n",
    "#Sort Values of dot_product2 by high to low\n",
    "print(\"\\nSorted dot product values\")\n",
    "print(dot_product2.sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Details - Standford's GloVe pre-trained word vectors\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus. The GloVe pre-trained word vectors dataset contains English word vectors pre-trained on the combined Wikipedia 2014 + Gigaword 5th Edition corpora (6B tokens, 400K vocab). All tokens are in lowercase. This dataset contains 50-dimensional, 100-dimensional and 200-dimensional pre trained word vectors. In this problem we are going to use the 50-dimensional dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\# 0. Get an overview on what Glove is\n",
    "Read up the documentation on glove embeddings, esp. where it gets applied here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "Let's load the dataset first. Each row is indexed as a word vector. Dimension of word vectors is 50. How many words are there in this dataset? Print a few words and see what they are. You don't need to code anything here, just understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qingchuanhou/opt/anaconda3/envs/main/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/qingchuanhou/Desktop/glove/glove.6B.50d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nl/jzl2sdyd5g7gv2bk8lfg29tr0000gn/T/ipykernel_33490/2508150459.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlocal_file1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/Users/qingchuanhou/Desktop/glove/glove.6B.50d.txt\"\u001b[0m \u001b[0;31m# Make sure this file exists!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_file1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE_NONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset shape - Rows: %d, Cols: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/main/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/main/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/main/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/main/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/main/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/main/lib/python3.9/site-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"readline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/main/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/main/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/qingchuanhou/Desktop/glove/glove.6B.50d.txt'"
     ]
    }
   ],
   "source": [
    "# Load GloVe pre-trained vectors \n",
    "local_file1=\"/Users/qingchuanhou/Desktop/glove/glove.6B.50d.txt\" # Make sure this file exists!\n",
    "data = csv.reader\n",
    "df = pd.read_csv(local_file1,sep=' ',index_col=0,header=None,engine='python',error_bad_lines=False, quoting=csv.QUOTE_NONE)\n",
    "print(\"dataset shape - Rows: %d, Cols: %d\" % (df.shape[0], df.shape[1]))\n",
    "words = list(df.index)\n",
    "print(\"print a few words in the dataset:\", words[30:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\# 1. Print the first few 11 rows of the pandas data frame below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code HERE - It should execute as expected! \n",
    "# (Search for a pandas functionality that can help you do this!)\n",
    "df.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\# 2. Words Similarity\n",
    "\n",
    "Similar words have similar embeddings (or vector values). We can use cosine similarity i.e. cos(u,v) = u.v/(|u||v|) to measure vector similarity. u.v is dot product of vectors, |u| is L2 norm of u. Remember, we spoke about computing similarity based on cosine-similarity (as another alternative to correlation) in class?\n",
    "\n",
    "1. Normalize matrix df by norm of word vectors. \n",
    "1. Define a function to find words similarity to a given word.\n",
    "1. Use the function defined to find the word in examples that is most similar to \"happy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "# 1a. Calculate norm of word vectors\n",
    "# What would be the dimension of the vector_norm array?\n",
    "vector_norm = np.sqrt(np.square(df).sum(axis=1))\n",
    "\n",
    "# 1b. Normalize matrix df by norm using .div()\n",
    "dfn = df.div(vector_norm, axis=0)\n",
    "\n",
    "# 2. Define a function to find words similar to a given word in a normalized dataframe\n",
    "def find_word_similarity(word, examples, dataframe):\n",
    "    # Input: word - one string\n",
    "    #        examples - List of strings\n",
    "    #        dataframe - An indexed normalized dataframe\n",
    "    ## YOUR CODE HERE\n",
    "    # Calculate dot product of each word in examples to the given word, sorted by value high to low\n",
    "    # Once you have the sorted values of dot products (notice because of normalization, the dot product is the cosine similarity!),\n",
    "    # obtain the words corresponding to the sorted values and call it similar_words\n",
    "    df = dataframe\n",
    "\n",
    "    dot_product = df.loc[examples].dot(df.loc[word])\n",
    "\n",
    "    similar_words = dot_product.sort_values(ascending = False)\n",
    "\n",
    "    # Return words similar to the given word\n",
    "    return similar_words\n",
    "    \n",
    "examples = [\"sad\", \"bad\", \"evil\", \"healthy\", \"ill\",\n",
    "            \"beaming\", \"cheerful\", \"joyful\", \"radiant\", \"glad\", \"upset\",\n",
    "            \"disco\", \"probably\", \"hardly\", \"ephemeral\", \"close\", \"cleaning\", \n",
    "            \"maths\", \"word\", \"distribution\"]\n",
    "\n",
    "# 3.\n",
    "# Use above function to calculate examples' similarity to happy (both \"happy\" and words in examples are in dfn)\n",
    "print(find_word_similarity(\"happy\", examples,dfn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklean library,there is a cosine_similarity fuction that directly calcualtes vectors similarity (you don't need to normalize vectors first). Let's use this function to calculate similarity again to confirm we get same results. \n",
    "For more information, please see here: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity2 = pd.DataFrame(cosine_similarity(np.array(df.loc['happy']).reshape(1,50),np.array(df.loc[examples])),columns = examples)\n",
    "print(similarity2.T.sort_values(by=0, ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\# 3. Goodness of similarity\n",
    "Comment on the how good the glove embeddings are on finding similar words to a given word using cosine similarity? Glove and word2vec embeddings are based on co-occurence of words in senetences across hundreds of thousands of documents on the web. Would this help explain your observations on word similarity?\n",
    "What if you replace happy with sad, how do the results change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples2 = [\"happy\", \"bad\", \"evil\", \"healthy\", \"ill\",\n",
    "            \"beaming\", \"cheerful\", \"joyful\", \"radiant\", \"glad\", \"upset\",\n",
    "            \"disco\", \"probably\", \"hardly\", \"ephemeral\", \"close\", \"cleaning\", \n",
    "            \"maths\", \"word\", \"distribution\"]\n",
    "print(find_word_similarity(\"sad\", examples2,dfn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result for using Glove embedding to find the similar words is pretty good. There are some words that don't seem to be very related, such as 'hardly' is similar for both 'happy' and 'sad'. I think this may also have something to do with the frequency of words, because all the data are normalized, so although some words are not very frequent, they appear in sentences containing other words, which will cause these words to use cosine have high similarity.Also, in the Glove, what we mean by similar is not just semantically similar, but also the similarty in the kind of words, like 'happy' and 'sad'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\# 4. Correlations\n",
    "(This question is more of a reflection and building your intuition on how correlations we spoke in class connects to a real-world data set -  Open ended!)\n",
    "What are some of the most correlated words from the similarity search you did earlier to the word \"happy\" and \"sad\". Likewise, what are some of the most uncorrelated words to \"happy\" and \"sad\". Does this make sense? How would you improve the results ? If \"happy\" were a random variable and \"sad\" was a random variable - What factors make the correlation between \"happy\" and \"sad\" (as you computed above) high?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Glove, what we mean by similar is not just semantically similar, but also the similarty in the kind of words. Like 'happy' and 'sad'. It make sense because these words can be exchanged in some sentences. The other words in this sentence will influce this word's vector in Glove. like 'I feel sad' and 'I feel happy' because the influence of the words I and feel makes 'sad' and 'happy' have similar vectors in Glove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\# 5. Find nearest neighbourhood\n",
    "\n",
    "It is helpful to compute the nearest neighbors to a word based on the cosine similarity that we defined earlier, so that given a word we can compute which are the other words which are most similar to it. Sometimes, the nearest neighbors according to this metric reveal overlap of concepts or topics that a word shares. E.g. government might be related to the word politics because they both share topics related to public policy, politicians, parties, elections, etc. The idea is whatever embeddings we are using - word2vec or glove is \"hopefully\" able to capture these correlations right!\n",
    "\n",
    "1. Define a function to find the top n similar words to a given word. You can use either dot product of vectors or cosine_simialrity function. Note the search space for words is coming from your pandas data frame (so unlike the similarity problem we worked on earlier, we are not restricted to only a few words to search from - the search space here is the entire vocab captured in your data frame).\n",
    "1. Find 20 nearest neighborhood for words 'duck' and 'animal'.\n",
    "1. Find neighborhood intersection of 'duck' and 'animal', to find which words are similar to both 'duck' and 'animal'. This is also related to a similarity measure called \"Jaccard Similarity\" - Read up on this here: https://en.wikipedia.org/wiki/Jaccard_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = list(dfn.index)\n",
    "lis.remove('the')\n",
    "print(lis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to find the top n similar words to a given word in the 'df'\n",
    "\n",
    "# PART 1\n",
    "def find_most_similar(df, word, n):\n",
    "    # INPUT: \n",
    "    # df: Given Data frame\n",
    "    # word: String\n",
    "    # n: Number of similar words to return\n",
    "    \n",
    "    # OUTPUT:\n",
    "    # the list of similar words to return\n",
    "    \n",
    "    ## YOUR CODE HERE\n",
    "    # define and compute the most similar words\n",
    "    # Use a similarity measure like cosine similarity (like earlier) to do so\n",
    "\n",
    "    words = list(df.index)\n",
    "    words.remove(word)\n",
    "\n",
    "    dot_product = df.loc[words].dot(df.loc[word])\n",
    "\n",
    "    similar_words = dot_product.sort_values(ascending = False).head(n)\n",
    "    \n",
    "    return similar_words\n",
    "\n",
    "\n",
    "# PART 2\n",
    "# find top 20 similar words to duck\n",
    "simil1 = find_most_similar(dfn, \"duck\", 20)\n",
    "print('Simillar words to \\'duck\\' (most simillar first):', list(simil1.index))\n",
    "print()\n",
    "# find top 20 similar words to animal\n",
    "simil2 = find_most_similar(dfn, \"animal\", 20)\n",
    "print('Simillar words to \\'animal\\' (most simillar first):', list(simil2.index))\n",
    "\n",
    "# PART 3\n",
    "# find the intersection of simil1 and simil2\n",
    "# intersection =  (concat function of pandas is helpful here)\n",
    "# print intersection\n",
    "intersection = pd.concat([simil1, simil2], axis=1, join='inner')\n",
    "print(intersection)\n",
    "\n",
    "\n",
    "word_labels = ['duck', 'animal'] + list(intersection.index)\n",
    "print(word_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\# 6 Word analogies\n",
    "\n",
    "Suppose you know the word vectors for King, Man and Woman. What is your intuitive answer for the 'riddle', King - Man + Woman = ? \n",
    "Let's go through below steps to derive the answer for this 'riddle' using the word embeddings.\n",
    "\n",
    "1. Use vector arithmetic to define a new vector which equals to k - m + w (e.g. king, man and woman combination).\n",
    "2. Calculate similarity of all the words in the corpus to the new vector and sort them by their similarity high to low. \n",
    "3. Return the top n vectors which have the highest similarity to the new vector.\n",
    "1. Find the answers for the riddles, \n",
    "    1. good:bad::up:?\n",
    "    1. germany:merkel::america:?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to solve the problem of x is to y as a is to ?\n",
    "# 'n' is the number of top words similar to the vector to return\n",
    "# 'dataframe' is the indexed dataframe that contains all the words\n",
    "\n",
    "# PART 1,2,3 above (Fill in the missing pieces)\n",
    "def solve_riddle(x, y, a, n, dataframe):\n",
    "    ## YOUR CODE HERE\n",
    "    # calculate the vector of a + y - x, where a, x, y are in dataframe\n",
    "    df = dataframe\n",
    "    cal_vec = df.loc[a] + df.loc[y] - df.loc[x]\n",
    "    \n",
    "    # calculate distance of words in dataframe to cal_vec, sorted by similarity high to low\n",
    "    dot_product = df.loc[list(df.index)].dot(cal_vec)\n",
    "\n",
    "    similarity = dot_product.sort_values(ascending = False).head(n)\n",
    "\n",
    "    # return top n words and distance that closest to cal_vec\n",
    "    return similarity \n",
    "\n",
    "# Call the solve_riddle function to compute the top answers\n",
    "print(solve_riddle(\"man\", \"woman\", \"king\", 5,df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "# Solve the other two riddles\n",
    "# good:bad::up:?\n",
    "# PART 4\n",
    "#print solve_riddle()\n",
    "print(solve_riddle(\"good\", \"bad\", \"up\", 5,df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# germany:merkel::america:?\n",
    "#print solve_riddle()\n",
    "print(solve_riddle(\"germany\", \"america\", \"merkel\", 5,df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

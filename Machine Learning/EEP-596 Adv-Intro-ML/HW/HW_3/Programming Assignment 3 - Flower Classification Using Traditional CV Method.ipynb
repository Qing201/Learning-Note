{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e25ed18",
   "metadata": {},
   "source": [
    "## Flower Classification\n",
    "In this programming assignment, we will be looking at Flower Classification with a real dataset which has a 'label' for every flower image. The images are divided into five classes: chamomile, tulip, rose, sunflower, dandelion.\n",
    "For each class there are about 800 photos. Photos are not high resolution, about 320x240 pixels. Photos are not reduced to a single size, they have different proportions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b95ee1",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23e7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # First install your OpenCV-Python if you haven't\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18853ebb",
   "metadata": {},
   "source": [
    "## Loading the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a11785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1074.625] global /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('./Users/qingchuanhou/Desktop/flower_train/daisy/99306615_739eb94b9e_m.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nl/jzl2sdyd5g7gv2bk8lfg29tr0000gn/T/ipykernel_34493/3358177417.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./Users/qingchuanhou/Desktop/flower_train/daisy/99306615_739eb94b9e_m.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Image will be B,G,R order.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msample_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# plt.imshow() will assume R, G, B order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.5) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/Users/qingchuanhou/Desktop/flower_train/daisy/99306615_739eb94b9e_m.jpg\"\n",
    "sample_image = cv2.imread(image_path) # Image will be B,G,R order.\n",
    "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(sample_image) # plt.imshow() will assume R, G, B order\n",
    "plt.show()\n",
    "print(\"Image Size: {}\\n\".format(sample_image.shape),\n",
    "      \"Image Data Type: {}, {}\\n\".format(type(sample_image), sample_image.dtype),\n",
    "      \"Max-Min Pixel Value: {}, {}\".format(sample_image.max(), \n",
    "                                           sample_image.min()))\n",
    "\n",
    "# Basic processing method - resize, normalization\n",
    "sample_image_resize = cv2.resize(sample_image, (224, 224))\n",
    "sample_image_norm_resize = sample_image_resize / 255.0\n",
    "print(\"After normalization, Max-Min Pixel Value: {}, {}\".format(sample_image_norm_resize.max(), \n",
    "                                           sample_image_norm_resize.min()))\n",
    "plt.imshow(sample_image_norm_resize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9303192",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "For feature extraction, you can use from the most simple statistics of the image, e.g., statistic of colors, histogram of intensity/color values, to many advanced feature extractors/descriptors(shape, texture) in traditional image processing and computer vision, e.g, SIFT(scale-invariant feature transform), SURF(speeded up robust features), LESH(Local energy-based shape histogram), Gabor filters. These feature descriptors are built into Open-CV Python package, you can directly use them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04514dc",
   "metadata": {},
   "source": [
    "### SIFT feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "016a70f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224)\n",
      "(80.62425994873047, 128.3155517578125) [ 37.   4.   2.  25. 129.  10.   0.   0. 140.  14.   0.   1.  28.  14.\n",
      "   3.  13.  38.   2.   0.   0. 133.  64.   6.  27.  29.   5.   3.  21.\n",
      "  74.  24.  10.  33.  38.   7.   8.  14. 108.  17.   0.   1. 140.  27.\n",
      "   5.  14.  38.  15.   4.  39.  57.  18.   9.  78. 140.  48.   5.  19.\n",
      "  18.  19.   8.  53.  98.   2.   0.   2.  21.  15.   3.  12.  80.  14.\n",
      "   2.   4. 140. 140.  70.  56.  34.   7.   0.   3.  20.  52.  77. 140.\n",
      "  52.   0.   0.   3.   7.  34.  29.  48.  47.   1.   0.   3.   9.   2.\n",
      "   1.  25.  42.  37.   8.   7.   4.  26.  32.  30.  43.  18.   2.   4.\n",
      "  19.  17.  27.  25.   0.   1.  10.  13.  11.   9.   5.  15.  28.   5.\n",
      "   4.   6.]\n",
      "(56.59153366088867, 64.57383728027344) [  1.   0.   2.   9.  10. 121.  73.   4.  57.   1.   1.   1.   2.  36.\n",
      "  77. 122.  24.   0.   0.   1.  43.  60.  17.  38.   0.   0.   0.   0.\n",
      "  10.  81.  59.   2.  28.  16.  27.  65.  50.  52.  16.   9. 122.  31.\n",
      "  11.   7.   8.  45.  78. 122.  14.   3.   2.  16.  94. 122.  50.  26.\n",
      "   1.   2.   1.   2.  13.  26.  43.  11.  17.   3.   1.   4.  25. 122.\n",
      " 110.  21. 122.  47.  24.  32.  12.  22.  54.  65.  17.  22.  35. 122.\n",
      " 122.  45.   5.   4.   9.  38.  24.  32.  19.  10.   7.   8.   0.   0.\n",
      "   0.   0.  13.  63.  57.   1.   3.   3.   3.  31.  64.  37.  18.   6.\n",
      "   1.   2.   4.  53.  71.  30.  18.   5.   7.   8.   3.   5.   7.  17.\n",
      "  44.  26.]\n",
      "(138.18331909179688, 50.32956314086914) [  6.  39.  51.   6.   0.   1.   2.   4.   3.  24. 126.  20.   1.   0.\n",
      "   0.   2.   2.  50.  53.   5.   3.   2.   2.   0.   1.   4.   2.  24.\n",
      "  38.   1.   0.   0.   0.  17. 109.  94.  20.   1.   0.   0.  15.  53.\n",
      " 135. 135.  10.   7.  10.   4. 135. 135. 102.   7.   2.   1.   3.  10.\n",
      "  33.  17.   1.  30.  75.   7.   0.   0.  12.   7.  27.  89.  92.   7.\n",
      "   3.   5.  47.  10.  19.  41.  27.  28. 106.  61. 135.  42.   5.   3.\n",
      "   2.   6.  38.  71.  50.  11.   1.  14.  32.  16.   5.   5.  51.  12.\n",
      "  47.  73.   8.   1.   2.  22. 106.  15.   4.   5.   3.   5.  14.  33.\n",
      " 135.  45.   1.   2.   2.   4.   8.   9.  46.  34.   5.   2.   1.   3.\n",
      "   4.   8.]\n",
      "(124.23341369628906, 195.32423400878906) [ 11.   4.   1.   2.   2.   2.   3.  10.   3.   0.   1.   2.   4.  15.\n",
      "  36.  36.   1.   0.   1.   4.   8.  37.  59.  30.   3.   0.   0.   0.\n",
      "   0.   0.  91.  73.  36.   1.   0.   0.   8.  12.   4.  18. 123.   6.\n",
      "   1.   2.   7.  42.  68. 123.  39.  10.   2.  10.  30. 123. 123. 115.\n",
      "  41.  17.   9.   1.   0.   1. 123. 123.  62.   5.   1.  14.  23.   5.\n",
      "   0.   1. 123.  82.  18.  36.  13.   6.   2.  15.  58.  90.  31. 123.\n",
      "  66.  21.   5.  11.  62. 123.  87.  27.   0.   0.   4.  14.  29.   6.\n",
      "   1.  11.   3.   2.   4.   9.  67.  67.  25.  30.   3.   2.   2.   8.\n",
      "  13.  48.  26.  96.  14.   5.   6.   5.  16.  81.  53.  16.   0.   0.\n",
      "   2.   5.]\n",
      "(175.3207244873047, 202.39500427246094) [ 26. 126.  13.   0.   0.   0.   0.   0.  46. 126.  22.   0.   0.   0.\n",
      "   0.   2.  51.  27.   9.   3.   0.   0.   3.  17.   9.  10.   4.   1.\n",
      "   1.   5.  12.  18.  45. 103.   6.   1.   0.   0.   1.   7. 126. 126.\n",
      "  22.   5.   0.   0.   0.  19. 108.  45.  47. 113.   3.   0.   0.  15.\n",
      "  69.  80.  26.  20.   1.   1.   1.   1.  37.   6.   0.   0.   0.   0.\n",
      "   2.  34. 126. 115.  70.  18.   0.   0.   0.   9.  49.  99. 126. 126.\n",
      "   1.   0.   0.   1.  80. 126.  82.  35.   0.   0.   1.   1.   6.   4.\n",
      "   5.   1.   0.   0.   0.  12.   5.  22.  42.   4.   0.   0.   2.  12.\n",
      "  14.  38.  44.  11.   0.   0.  27.  50.  26.  56.  31.   9.   0.   0.\n",
      "  22. 102.]\n",
      "(109.9334716796875, 57.024879455566406) [  3.   4.  53.  87.  40.  10.   5.   3.  16.  36. 113.  23.   1.   2.\n",
      "   5.   6.   3.  10. 126.  52.   3.   7.   4.   1.   1.  11. 126.  51.\n",
      "   2.   2.   0.   0.  26.  20. 126. 126.   7.   1.   0.   1. 126.  65.\n",
      " 113.  21.   1.   7.  23.  65.  18.   4.  29.  39.  40. 107.  50.  28.\n",
      "   2.   9.  17.  15.  21.  48.  12.   1.  75.  10.  18.  67.  24.   1.\n",
      "   5.  33. 126. 123.  58.  17.   1.   1.   3.  31.  21.  36. 125. 114.\n",
      "  26.  17.   5.   6.   5.   6.  18.  29.  26.  13.   1.   0.  35.   3.\n",
      "   1.   6.   4.   1.   3.  50.  35.  59.  52.   6.   3.   1.   1.  11.\n",
      "  53.  45.  62.  16.   0.   0.   0.   3.  21.  15.  18.  43.   7.   0.\n",
      "   0.   0.]\n",
      "(86.4204330444336, 25.917949676513672) [ 27.  10.  15.  27.   1.   0.  66. 122.  96.  20.  10.   8.   4.   2.\n",
      "  31. 122.  75.  50.  17.   4.   2.   0.   0.  19.   6.   7.   2.   2.\n",
      "  15.  16.  11.   4.   0.   0.   0.   2.  24.  12. 122. 122.   7.  19.\n",
      "  42. 115. 122.  41.  47.  46. 122. 122.  62.  35.  11.   7.   6.  23.\n",
      "  52.  27.   4.   4.   4.   2.   2.   3.   6.   3.  10.  18.  35.  17.\n",
      "  12.  38.   6.   2.   6.  31. 122. 122.  67.  21. 111.  15.   3.   5.\n",
      "   8.  34. 122. 122.  37.  21.   7.   0.   1.   4.  11.  11.   4.   4.\n",
      "  32.  60.  25.   2.   0.   2.  22.  34.  37.  30.  11.  10.  12.   9.\n",
      "  22.  35.  22.   1.   1.  13.  45.  26.   1.   6.  11.   1.   1.  18.\n",
      "  20.   2.]\n",
      "(86.4204330444336, 25.917949676513672) [  1.  17.   7.   1.   4.   9.   2.   0.   3.  33.  50.  15.   9.  15.\n",
      "   6.   1.   3.   8.  25.  16.  45.  40.  16.   5.  16.   6.   2.   1.\n",
      "   8.  16.  48.  37.   3.   3.   1.  13.  45.  16.   3.   3.   9.  32.\n",
      " 112. 118. 111.  13.   3.   5. 118. 115.  80.  20.   5.   3.  11.  34.\n",
      "  37.   5.   3.   5.   2.   3.  45.  31.   2.   2.   3.   9.  56.  21.\n",
      "   2.   3.  11.   6.   7.  33. 118. 118.  59.  43. 118.  33.  60.  17.\n",
      "   4.  12.  36. 118.  32.  10. 118.  81.   1.   0.   1.   6.  15.   3.\n",
      "   1.  13.  22.   7.   1.   7.   1.   0.   9.  81.  75.  27.  10.   4.\n",
      "   5.   6. 118. 118.  33.   5.   7.  10.   0.   1. 118. 118.   3.   2.\n",
      "  14.   8.]\n",
      "(106.9581298828125, 127.76534271240234) [ 10.  12.  65.  15.   0.   0.  11.  65.  42.  15.  61.  29.   0.   0.\n",
      "   1.  53.  38.   7.  13.  34.  11.  11.  13.  76.  26.   7.   7.  21.\n",
      "   6.  19.  43.  15.  25.  18. 131.  71.   0.   0.   3.  10. 131.  60.\n",
      "  61.  26.   4.   0.   0.  59.  71.  19.  16. 131.  76.   0.   1.  52.\n",
      " 131.  38.  15.  47.  11.   1.   1.   5.  37.   4.  13.   8.   0.   0.\n",
      "  51.  56. 131.  64.  24.  28.   5.   1.  14.  71.  48.  22.  25. 131.\n",
      "  52.  13.  51.  39. 131.  23.   3.   9.   5.   3.  16.  45.   2.   1.\n",
      "   3.   6.   4.  18. 131.  46.  12.   5.   3.   1.   1.   5.  72.  97.\n",
      "   5.  12.   5.   2.   5.  14.  66.  45.  18.  32.  13.   9.   6.   1.\n",
      "  17.  24.]\n",
      "(106.9581298828125, 127.76534271240234) [  1.  16.  25.  20.  11.   8.  13.   2.   0.   0.  10.  92.  66.  12.\n",
      "   5.   1.   0.   0.  22. 130.  75.  22.   1.   0.   9.   0.   1.  35.\n",
      "  12.   4.   7.  46.   7.   5.  11.  89.  50.  20.  16.   2.  97.  13.\n",
      "  18.  58.  40.  63.  33.  35.  12.   0.   0.   5. 104. 130.  42.  28.\n",
      "  23.   0.   0.   4.  13.  25.  21. 130.   9.   3.   4.  16.  75. 130.\n",
      "  10.   3. 130.  20.   2.   6.  30.  80.  16.  32.  64.   8.   0.   8.\n",
      " 130. 130.  25.  32.  10.   0.   0.   9.  45.  30.  28. 119.  20.   7.\n",
      "   4.   4.  14.  65.   3.   6.  72.   6.   5.   7.  23.  80.  18.  31.\n",
      "  34.   6.  12.  16.  91.  20.   7.  18.   6.   1.   2.   2.  25.  16.\n",
      "  18.  50.]\n",
      "(50.075374603271484, 98.49798583984375) [ 17.  11.  12.  34.  63.  72.  10.  16.  44.  39.   4.   4.  31.  80.\n",
      "  21.  40.   1.   1.   0.  13.  67. 122.  44.   0.   0.   2.   3.   4.\n",
      "  43.  47.  28.   0.  36.  39. 122. 122.  23.   5.   0.   2. 122. 122.\n",
      "  30.  17.   9.  36.   5.  20.  25.   6.  13.  45. 122. 122.  19.   6.\n",
      "   3.   1.   1.   6.  76.  44.  81.  26.  94.  23.  39.  15.   0.   0.\n",
      "   9.   8. 122. 113.  70.  40.   2.   1.   1.  13.  19.  20.  93. 122.\n",
      "  33.   8.  11.  11.  31.  51.  12.   9.  10.   9.  27.  30.  20.   7.\n",
      "   0.   0.   1.  23.  90.   9.  67.  58.  10.   4.   1.   8.  14.  11.\n",
      "  12.  21.  28.   8.   1.   1.  13.  12.  10.  29.  19.   4.   2.   0.\n",
      "   2.   4.]\n",
      "(118.51445770263672, 135.42889404296875) [ 13.  22.   4.   4.   8.  13.  20.   5.   1.  27.  21.  19.   6.  31.\n",
      "  32.   2.   1.  33.  72.  24.  15.  18.   3.   0.   0.  27. 118.  27.\n",
      "  19.  20.   1.   0.  30.  38.  50.  26.   4.   1.   4.  10.  11.  23.\n",
      "  96. 118.  50.  13.   5.   1. 118.  78.  49.  31.  27.  50.   9.  10.\n",
      "  11.  14.  40.  29. 118. 112.   5.   2.   2.   8.  45.  44.  33.  20.\n",
      "   3.   4.  43.  21.  51. 106. 116.  15.   0.   4. 118.  67.  32.  52.\n",
      "  36.   5.   1.  20.  14.   5.  14. 118. 118.  10.   1.   1.   1.   1.\n",
      "  10.  88. 118.  20.   5.   4.  37.  21.  38. 107.  21.   0.   0.   4.\n",
      "  74.  71.  47.  24.   5.   2.   3.  16.   4.   3.  21.  48.  54.   6.\n",
      "   9.  14.]\n",
      "(107.915283203125, 220.5498504638672) [  0.   0.   0.   0.   1.   0.   0.   0.  28.   2.   0.   0.   1.   0.\n",
      "   0.   2.  18.   3.  10.   6.   0.   0.   0.   1.   0.   0.  19.  55.\n",
      "   3.   0.   0.   0.  13.  18.  19.  80.  37.   0.   0.   1. 132.  52.\n",
      "  11.  32.  23.   1.   0.  38.  89.  25.  83. 132.  16.   0.   0.  10.\n",
      "   0.   0. 132. 132.   7.   0.   0.   0.   8.  43.  91. 127.   5.   0.\n",
      "   0.   0.  90. 132.  75.  33.   2.   0.   0.   3.  13. 117. 113. 132.\n",
      "  67.   4.   0.   1.   0.   1.  33. 103.  82.  29.   0.   0.   3.   8.\n",
      "  17.  11.   8.   4.   1.   2.   5.  88.  17.   2.   2.   1.   0.   0.\n",
      "   2. 123.  67.   8.   7.   1.   0.   0.   1.  18.  30.  12.  25.  22.\n",
      "   0.   0.]\n",
      "(97.17681121826172, 137.22535705566406) [  2.  13.  68.  78.  17.   0.   0.   0.  95.  53.  70.  47.  14.   0.\n",
      "   0.  26. 106.  19.   5.   3.  10.  17.   7.  32.   6.   3.   5.  12.\n",
      " 125. 106.   4.   3.  18.  14.  42.  84.  22.   6.   1.   2. 136.  12.\n",
      "   5.  34.  11.   3.   5.  97.  85.  10.  19.  52.  46.  18.  14.  58.\n",
      "   4.   5.  40.  95.  76.  33.   0.   1.  24.   2.   2.  12.  70.  38.\n",
      "   2.   2. 136.  86.   4.   8.  10.   5.   1.  10.  67.  39.  10.  83.\n",
      " 128.  10.   1.   4.   6.   5.  18.  41.  53.  12.   0.   1.   6.   2.\n",
      "   4.  32.  46.   9.   1.   1. 128. 114.   6.   2.   9.   4.   1.   1.\n",
      "  27.  92.  40.  16.  39.   9.   0.   0.   0.   7.  10.   7.  47.  38.\n",
      "   0.   0.]\n",
      "(121.91979217529297, 137.72286987304688) [  5.   2.   6.  41.  43.  53.   4.   3.  26.   7.   2.   6.  25.  38.\n",
      "  34.  35.  23.   1.   0.   0.  41.  89.  28.  51. 118.  25.   3.   2.\n",
      "   1.   2.   5.  74.  35.  12.  21. 118.  32.   1.   4.   6. 118.  25.\n",
      "   6.   7.  14.  33.  28.  77.  57.   7.   1.   0.  91. 118.  44.  64.\n",
      "  82.   9.   0.   0.   3.   9.  21. 118.  30.   2.   0.  26.  13.   2.\n",
      "  42.  53. 118. 107.   7.   1.   8.   6.  53.  49.  43.  60.  13.   2.\n",
      "  47.  75.  50.  61.  32.   8.  16.   2.   2.  12.  32.  99.  15.   0.\n",
      "   0.   0.   0.   3.  68. 103.  38.  14.   2.   0.   0.   0. 105. 112.\n",
      "  16.  23.   7.   0.   0.   3.  76.  71.   2.   8.  11.   1.   1.  22.\n",
      "  57.   5.]\n",
      "(121.91979217529297, 137.72286987304688) [  7.  20.  18.  16.   6.  21.  14.   1.   1.  45.  66.  26.  10.  11.\n",
      "   3.   0.   0.  18. 117.  39.  21.  18.   0.   0.   0.  19.  89.  57.\n",
      "   9.   2.   0.   0.  10.  15.  76. 113.  27.   2.   1.   1. 117.  60.\n",
      "  63.  63.  38.  23.   3.  11.  18.  19.  83.  74. 117.  55.   2.   4.\n",
      "   0.  25. 117.  67.  14.   0.   0.   1.  19.  15.  61. 117.  48.   2.\n",
      "   0.   1. 117. 100.  51.  72.  26.   1.   0.   9.  13.  11.  27. 117.\n",
      " 117.   9.  11.   6.   3.   7.  26.  25.  17.   2.  29.  80.  39.  14.\n",
      "  14.  69.  54.   1.   0.   2.  72.  51.  36.  18.   8.   0.   0.   9.\n",
      "  27.  11.  15.  31.  32.   7.  32.  42.  20.   3.   0.   0.   2.   2.\n",
      "  61. 102.]\n",
      "(147.664794921875, 81.66761016845703) [ 29.  14.   2.   0.   1.   6.  24.  26.   1.   0.   0.   5. 110.  80.\n",
      "  31.   6.  65.   2.   0.   2.  94.  51.  47.  65. 107.   9.   5.   0.\n",
      "   0.  11.  76.  64. 115.  33.   3.   5.  13.   4.   5.  29.  26.   5.\n",
      "   3.  85. 115.  46.  15.  14. 115.  13.   2.  19.  35.  26.  86. 115.\n",
      "  33.  10.  10.   2.  39.  45.  59.  48. 115.   4.   1.   1.   6.  10.\n",
      "   6.  76.  19.  10.   3.  11.  50. 115. 108.  22. 115.   8.   1.   2.\n",
      "   7.  33.  90. 115.  18.   1.   0.   0.  41.  60.  19.  19. 111.  10.\n",
      "   5.   6.   4.   0.   1.  23.   7.  58.  23.   8.   5.  10.  22.   3.\n",
      "  14.  30.  20.   3.   1.   6.  22.  15.   4.  10.  13.   3.   2.  21.\n",
      "   9.   3.]\n",
      "(78.87181091308594, 107.40870666503906) [  6.   1.   9.  98.  18.   1.   5.  24.   5.  10.  61. 125.  30.  19.\n",
      "  11.   3.  56.  73.  31.  21.   2.  17.  17.  13.  13.  20.   3.   5.\n",
      "  33.  72.  24.  10.  17.   7.  17. 125.  58.  23.   0.   1. 125.  17.\n",
      "  11.  42.  14.  71.  51.  42.  34.  96.  10.   1.  20. 125.  35.  10.\n",
      "   9.  88.  82.  57.  40.  34.   2.   2.  22.   2.   3.  14. 122. 125.\n",
      "   5.   7. 125.  49.   8.   8.  15.  40.  11.  25.  24.  20.  16.  28.\n",
      "  74. 125.   4.   3.   3.   6.  25.  14.  48. 125.   3.   1.   2.   4.\n",
      "  27.  14.  63.  59.   2.   1.  49.  48.   7.   3.  25.  41.   4.   4.\n",
      "  11.  30.  22.  21.  12.   6.   0.   0.   1.   0.   1.   8.  32.  68.\n",
      "   9.   1.]\n",
      "(78.87181091308594, 107.40870666503906) [ 16.   2.   8.  20.   9.  41.  41.  12. 127.   5.   1.   2.   2.  11.\n",
      "   9.  54.  17.   0.   0.   4.  16.  19.  24.  23.   5.   0.   1.  24.\n",
      "  29.   4.   2.  13.  36.   7.   3.  14. 101.  66.  46.  42. 127.  10.\n",
      "   3.  11.  46.  22.  10.  55.  36.   5.   8. 124.  67.  23.  24.  27.\n",
      "  66.   7.   6.  50.  34.   4.   3.  36.  25.  21.  17.  60.  88.  14.\n",
      "   7.   4. 127.  70.  26.  61.  45.   8.   9.   9.  38.  20.  33. 127.\n",
      "  27.   8.  27.  22. 127.   6.   7.  26.   2.   3.  10. 107.   2.   1.\n",
      "   2.   6.  11.  61.  99.  16.  14.  10.   4.   8.   7.  42. 127.  38.\n",
      "   7.   2.   3.  13.   7.  16. 127.  40. 104.   4.   1.   1.   1.   3.\n",
      "  36.  76.]\n",
      "(61.234554290771484, 99.72457885742188) [ 19.  33.   9.   9.  18.   8.  22.  26.  70.  83.  31.   6.   0.   1.\n",
      "  23.  78.  90. 102.   8.   7.  29.  43.  12.  59.  25.  12.   5.  18.\n",
      "  76. 109.   6.   6.  36.  27.   7.  23. 100.   9.   2.   3. 122.  53.\n",
      "  13.   5.   3.   2.  18. 122.  54.  33.  16.  58.  44.  16.  42.  95.\n",
      "   1.   4.  10.  68.  58.  21.   1.   0.  32.   1.   0.  17.  49.  17.\n",
      " 114.  39. 122.  80.  24.  13.   4.   7.  95.  76.  27.  30.  79. 116.\n",
      "  25.  15.   8.  12.   2.   4.  22.  29.  18.   5.   4.   6.  48.  27.\n",
      "  10.   3.   1.   4.  69.  48.  20.  18.  59.  84.   8.  10. 100.  42.\n",
      "  11.  29.  50.  54.  14.  17.  11.   8.  18.   9.   3.   1.   2.   8.\n",
      "  13.  18.]\n",
      "Number of keypoints: 20, Feature Dimension For each keypoints: (128,)\n"
     ]
    }
   ],
   "source": [
    "gray = cv2.cvtColor(sample_image_resize, cv2.COLOR_RGB2GRAY)\n",
    "print(gray.shape)\n",
    "sift = cv2.SIFT_create(20) #limit the number of keypoints you want to constrain\n",
    "\n",
    "kp, des = sift.detectAndCompute(gray,None)\n",
    "for i in range(len(kp)):\n",
    "    print(kp[i].pt, des[i])\n",
    "print(\"Number of keypoints: {}, Feature Dimension For each keypoints: {}\".format(len(kp), des[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02e27a",
   "metadata": {},
   "source": [
    "### Color Histogram feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c36406f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWiklEQVR4nO3df4xdd3nn8fdnY4jSQlJIBtbrH2tDDGoS7RpseSOxIFZpGxO6OHQJdVQRr2rJEAUtqF2pSZEWtJIl0i5EG2kxMpsoDqL50YQ0liBb0lCBKoWESTCxk5BmQgwZ7LXdEIFXQLY2z/5xv9M9Ht+ZsedO5s7g90u6uuc+53zPfe6ZsT9zzrn3nlQVkiT9s2E3IElaGAwESRJgIEiSGgNBkgQYCJKkZsmwG5itCy64oFatWjXsNiRpUXnsscf+oapG+s1btIGwatUqRkdHh92GJC0qSX4w1TwPGUmSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJKAU/ikcpIVwO3APwd+Ceysqv+e5PXAXcAqYD/wwap6qY25AdgKHAf+U1X9dauvA24DzgG+CnysqirJ2e051gEvAr9fVfvn7FVK82jV9V8Z2nPv//R7h/bcWvxOZQ/hGPDHVfWbwKXAdUkuAq4HHqqqNcBD7TFt3mbgYmAj8LkkZ7V17QC2AWvabWOrbwVeqqoLgZuAG+fgtUmSTsOMgVBVB6vq8TZ9FHgaWAZsAna1xXYBV7bpTcCdVfVyVT0PjAEbkiwFzq2qh6t33c7bJ42ZWNc9wGVJMuBrkySdhtM6h5BkFfA24BHgjVV1EHqhAbyhLbYMeKEzbLzVlrXpyfUTxlTVMeAnwPl9nn9bktEko0eOHDmd1iVJMzjlQEjyGuBe4ONV9dPpFu1Tq2nq0405sVC1s6rWV9X6kZG+394qSZqlUwqEJK+iFwZfqqovt/KhdhiIdn+41ceBFZ3hy4EDrb68T/2EMUmWAOcBPz7dFyNJmr0ZA6Edy78FeLqqPtuZtRvY0qa3APd36puTnJ1kNb2Tx4+2w0pHk1za1nnNpDET6/oA8PV2nkGSNE9O5QI57wA+BOxNsqfV/hT4NHB3kq3AD4GrAKrqySR3A0/Re4fSdVV1vI27lv//ttMH2g16gfPFJGP09gw2D/ayJEmna8ZAqKq/o/8xfoDLphizHdjepz4KXNKn/gtaoEiShsNPKkuSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElScyqX0Lw1yeEk+zq1u5Lsabf9E1dSS7Iqyc878z7fGbMuyd4kY0lubpfRpF1q865WfyTJqrl/mZKkmZzKHsJtwMZuoap+v6rWVtVa4F7gy53Zz03Mq6qPdOo7gG30rrG8prPOrcBLVXUhcBNw42xeiCRpMDMGQlV9k951jk/S/sr/IHDHdOtIshQ4t6oerqoCbgeubLM3Abva9D3AZRN7D5Kk+TPoOYR3Aoeq6tlObXWS7yT5RpJ3ttoyYLyzzHirTcx7AaCqjgE/Ac4fsC9J0mlaMuD4qzlx7+AgsLKqXkyyDvirJBcD/f7ir3Y/3bwTJNlG77ATK1eunHXTkqSTzXoPIckS4PeAuyZqVfVyVb3Yph8DngPeQm+PYHln+HLgQJseB1Z01nkeUxyiqqqdVbW+qtaPjIzMtnVJUh+DHDL6LeB7VfVPh4KSjCQ5q02/id7J4+9X1UHgaJJL2/mBa4D727DdwJY2/QHg6+08gyRpHp3K207vAB4G3ppkPMnWNmszJ59MfhfwRJLv0jtB/JGqmvhr/1rgfwJj9PYcHmj1W4Dzk4wBfwRcP8DrkSTN0oznEKrq6inq/7FP7V56b0Ptt/wocEmf+i+Aq2bqQ5L0yvKTypIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJODULqF5a5LDSfZ1ap9K8qMke9rtis68G5KMJXkmyeWd+roke9u8m9u1lUlydpK7Wv2RJKvm+DVKkk7Bqewh3AZs7FO/qarWtttXAZJcRO9ayxe3MZ9LclZbfgewDVjTbhPr3Aq8VFUXAjcBN87ytUiSBjBjIFTVN4Efn+L6NgF3VtXLVfU8MAZsSLIUOLeqHq6qAm4HruyM2dWm7wEum9h7kCTNn0HOIXw0yRPtkNLrWm0Z8EJnmfFWW9amJ9dPGFNVx4CfAOf3e8Ik25KMJhk9cuTIAK1LkiabbSDsAN4MrAUOAp9p9X5/2dc09enGnFys2llV66tq/cjIyGk1LEma3qwCoaoOVdXxqvol8AVgQ5s1DqzoLLocONDqy/vUTxiTZAlwHqd+iEqSNEdmFQjtnMCE9wMT70DaDWxu7xxaTe/k8aNVdRA4muTSdn7gGuD+zpgtbfoDwNfbeQZJ0jxaMtMCSe4A3g1ckGQc+CTw7iRr6R3a2Q98GKCqnkxyN/AUcAy4rqqOt1VdS+8dS+cAD7QbwC3AF5OM0dsz2DwHr0uSdJpmDISqurpP+ZZplt8ObO9THwUu6VP/BXDVTH1Ikl5ZflJZkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAafwXUbSYrXq+q8MuwVpUXEPQZIEGAiSpMZAkCQBBoIkqZkxEJLcmuRwkn2d2p8n+V6SJ5Lcl+Q3Wn1Vkp8n2dNun++MWZdkb5KxJDe3S2nSLrd5V6s/kmTV3L9MSdJMTmUP4TZg46Tag8AlVfWvgL8HbujMe66q1rbbRzr1HcA2etdZXtNZ51bgpaq6ELgJuPG0X4UkaWAzBkJVfZPetY67ta9V1bH28FvA8unWkWQpcG5VPVxVBdwOXNlmbwJ2tel7gMsm9h4kSfNnLs4h/CHwQOfx6iTfSfKNJO9stWXAeGeZ8VabmPcCQAuZnwDnz0FfkqTTMNAH05J8AjgGfKmVDgIrq+rFJOuAv0pyMdDvL/6aWM008yY/3zZ6h51YuXLlIK1LkiaZ9R5Cki3A7wJ/0A4DUVUvV9WLbfox4DngLfT2CLqHlZYDB9r0OLCirXMJcB6TDlFNqKqdVbW+qtaPjIzMtnVJUh+zCoQkG4E/Ad5XVT/r1EeSnNWm30Tv5PH3q+ogcDTJpe38wDXA/W3YbmBLm/4A8PWJgJEkzZ8ZDxkluQN4N3BBknHgk/TeVXQ28GA7//ut9o6idwH/Nckx4Djwkaqa+Gv/WnrvWDqH3jmHifMOtwBfTDJGb89g85y8MknSaZkxEKrq6j7lW6ZY9l7g3inmjQKX9Kn/Arhqpj4kSa8sP6ksSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCTiEQktya5HCSfZ3a65M8mOTZdv+6zrwbkowleSbJ5Z36uiR727yb27WVSXJ2krta/ZEkq+b4NUqSTsGp7CHcBmycVLseeKiq1gAPtcckuYjeNZEvbmM+l+SsNmYHsA1Y024T69wKvFRVFwI3ATfO9sVIkmZvxkCoqm8CP55U3gTsatO7gCs79Tur6uWqeh4YAzYkWQqcW1UPV1UBt08aM7Gue4DLJvYeJEnzZ7bnEN5YVQcB2v0bWn0Z8EJnufFWW9amJ9dPGFNVx4CfAOf3e9Ik25KMJhk9cuTILFuXJPUz1yeV+/1lX9PUpxtzcrFqZ1Wtr6r1IyMjs2xRktTPbAPhUDsMRLs/3OrjwIrOcsuBA62+vE/9hDFJlgDncfIhKknSK2y2gbAb2NKmtwD3d+qb2zuHVtM7efxoO6x0NMml7fzANZPGTKzrA8DX23kGSdI8WjLTAknuAN4NXJBkHPgk8Gng7iRbgR8CVwFU1ZNJ7gaeAo4B11XV8baqa+m9Y+kc4IF2A7gF+GKSMXp7Bpvn5JVJkk7LjIFQVVdPMeuyKZbfDmzvUx8FLulT/wUtUCRJw+MnlSVJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpmXUgJHlrkj2d20+TfDzJp5L8qFO/ojPmhiRjSZ5Jcnmnvi7J3jbv5naZTUnSPJp1IFTVM1W1tqrWAuuAnwH3tdk3Tcyrqq8CJLmI3uUxLwY2Ap9LclZbfgewjd41mNe0+ZKkeTRXh4wuA56rqh9Ms8wm4M6qermqngfGgA1JlgLnVtXDVVXA7cCVc9SXJOkUzVUgbAbu6Dz+aJInktya5HWttgx4obPMeKsta9OT6ydJsi3JaJLRI0eOzFHrkiSYg0BI8mrgfcBfttIO4M3AWuAg8JmJRfsMr2nqJxerdlbV+qpaPzIyMkjbkqRJ5mIP4T3A41V1CKCqDlXV8ar6JfAFYENbbhxY0Rm3HDjQ6sv71CVJ82guAuFqOoeL2jmBCe8H9rXp3cDmJGcnWU3v5PGjVXUQOJrk0vbuomuA++egL0nSaVgyyOAkvwb8NvDhTvnPkqyld9hn/8S8qnoyyd3AU8Ax4LqqOt7GXAvcBpwDPNBukqR5NFAgVNXPgPMn1T40zfLbge196qPAJYP0IkkajJ9UliQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQMGQpL9SfYm2ZNktNVen+TBJM+2+9d1lr8hyViSZ5Jc3qmva+sZS3Jzu7ayJGkezcUewr+rqrVVtb49vh54qKrWAA+1xyS5CNgMXAxsBD6X5Kw2ZgewDVjTbhvnoC9J0ml4JQ4ZbQJ2teldwJWd+p1V9XJVPQ+MARuSLAXOraqHq6qA2ztjJEnzZNBAKOBrSR5Lsq3V3lhVBwHa/RtafRnwQmfseKsta9OT6ydJsi3JaJLRI0eODNi6JKlryYDj31FVB5K8AXgwyfemWbbfeYGapn5ysWonsBNg/fr1fZeRJM3OQHsIVXWg3R8G7gM2AIfaYSDa/eG2+DiwojN8OXCg1Zf3qUuS5tGsAyHJryd57cQ08DvAPmA3sKUttgW4v03vBjYnOTvJanonjx9th5WOJrm0vbvoms4YSdI8GeSQ0RuB+9o7RJcAf1FV/yvJt4G7k2wFfghcBVBVTya5G3gKOAZcV1XH27quBW4DzgEeaDdJ0jyadSBU1feBf92n/iJw2RRjtgPb+9RHgUtm24skaXB+UlmSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoG/eoKSTojrbr+K0N77v2ffu8rsl4DQdKiNsz/mH/VeMhIkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEuDnEDQPfJ+4tDgMcgnNFUn+NsnTSZ5M8rFW/1SSHyXZ025XdMbckGQsyTNJLu/U1yXZ2+bd3C6lKUmaR4PsIRwD/riqHm/XVn4syYNt3k1V9d+6Cye5CNgMXAz8C+BvkrylXUZzB7AN+BbwVWAjXkZTkubVIJfQPAgcbNNHkzwNLJtmyCbgzqp6GXg+yRiwIcl+4Nyqehggye3AlRgI0qLhYcFfDXNyDiHJKuBtwCPAO4CPJrkGGKW3F/ESvbD4VmfYeKv9Y5ueXO/3PNvo7UmwcuXKuWhd+pXif8waxMDvMkryGuBe4ONV9VN6h3/eDKyltwfxmYlF+wyvaeonF6t2VtX6qlo/MjIyaOuSpI6BAiHJq+iFwZeq6ssAVXWoqo5X1S+BLwAb2uLjwIrO8OXAgVZf3qcuSZpHg7zLKMAtwNNV9dlOfWlnsfcD+9r0bmBzkrOTrAbWAI+2cxFHk1za1nkNcP9s+5Ikzc4g5xDeAXwI2JtkT6v9KXB1krX0DvvsBz4MUFVPJrkbeIreO5Sua+8wArgWuA04h97JZE8oS9I8G+RdRn9H/+P/X51mzHZge5/6KHDJbHuRJA3Or66QJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSMEeX0FxshnmZwf2ffu/QnluSpuMegiQJMBAkSY2BIEkCFlAgJNmY5JkkY0muH3Y/knSmWRAnlZOcBfwP4LeBceDbSXZX1VPD7WzuDfOEtiRNZ6HsIWwAxqrq+1X1f4E7gU1D7kmSzigLYg8BWAa80Hk8DvybyQsl2QZsaw//T5JnZvl8FwD/MMux822x9LpY+oTF0+ti6RMWT6+LpU+YptfcONB6/+VUMxZKIKRPrU4qVO0Edg78ZMloVa0fdD3zYbH0ulj6hMXT62LpExZPr4ulTxhOrwvlkNE4sKLzeDlwYEi9SNIZaaEEwreBNUlWJ3k1sBnYPeSeJOmMsiAOGVXVsSQfBf4aOAu4taqefAWfcuDDTvNosfS6WPqExdPrYukTFk+vi6VPGEKvqTrpUL0k6Qy0UA4ZSZKGzECQJAFnYCAs1K/ISLIiyd8meTrJk0k+1uqfSvKjJHva7Yph9wqQZH+Sva2n0VZ7fZIHkzzb7l835B7f2tlue5L8NMnHF8o2TXJrksNJ9nVqU27DJDe039tnklw+5D7/PMn3kjyR5L4kv9Hqq5L8vLNtPz9ffU7T65Q/7wW2Te/q9Lg/yZ5Wn79tWlVnzI3eCevngDcBrwa+C1w07L5ab0uBt7fp1wJ/D1wEfAr4z8Pur0+/+4ELJtX+DLi+TV8P3DjsPif97P83vQ/lLIhtCrwLeDuwb6Zt2H4XvgucDaxuv8dnDbHP3wGWtOkbO32u6i63QLZp35/3Qtumk+Z/Bvgv871Nz7Q9hAX7FRlVdbCqHm/TR4Gn6X2CezHZBOxq07uAK4fXykkuA56rqh8Mu5EJVfVN4MeTylNtw03AnVX1clU9D4zR+30eSp9V9bWqOtYefoveZ4eGboptOpUFtU0nJAnwQeCO+eil60wLhH5fkbHg/tNNsgp4G/BIK3207ZrfOuzDMB0FfC3JY+0rRQDeWFUHoRdwwBuG1t3JNnPiP7CFuE1h6m24kH93/xB4oPN4dZLvJPlGkncOq6lJ+v28F+o2fSdwqKqe7dTmZZueaYFwSl+RMUxJXgPcC3y8qn4K7ADeDKwFDtLblVwI3lFVbwfeA1yX5F3Dbmgq7cOO7wP+spUW6jadzoL83U3yCeAY8KVWOgisrKq3AX8E/EWSc4fVXzPVz3tBblPgak7842XetumZFggL+isykryKXhh8qaq+DFBVh6rqeFX9EvgC87RLO5OqOtDuDwP30evrUJKlAO3+8PA6PMF7gMer6hAs3G3aTLUNF9zvbpItwO8Cf1DtYHc7/PJim36M3nH5twyvy2l/3gtxmy4Bfg+4a6I2n9v0TAuEBfsVGe244S3A01X12U59aWex9wP7Jo+db0l+PclrJ6bpnWDcR29bbmmLbQHuH06HJznhL66FuE07ptqGu4HNSc5OshpYAzw6hP6A3rv1gD8B3ldVP+vUR9K7vglJ3kSvz+8Pp8t/6mmqn/eC2qbNbwHfq6rxicK8btP5OHO9kG7AFfTewfMc8Ilh99Pp69/S2119AtjTblcAXwT2tvpuYOkC6PVN9N6d8V3gyYntCJwPPAQ82+5fvwB6/TXgReC8Tm1BbFN6IXUQ+Ed6f61unW4bAp9ov7fPAO8Zcp9j9I6/T/yufr4t+x/a78R3gceBf78AtumUP++FtE1b/TbgI5OWnbdt6ldXSJKAM++QkSRpCgaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLU/D+zUZIWlposnAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "H = cv2.cvtColor(sample_image_resize, cv2.COLOR_BGR2HSV)[...,0]\n",
    "hue_dist, bins = np.histogram(H, bins=255)\n",
    "plt.hist(H.reshape((-1,)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9273cb",
   "metadata": {},
   "source": [
    "### Process the features into what you can fit into ML models\n",
    "Especially for feature like SIFT, you want to reduce dimensionality since (num_keypoints, 128) is very large. First you might want to limit the number of keypoints to extract. Also, you want to reduce the dims by a method called `bag of visual words`, where you treat each 128-d vector as a visual word, and assign it a label. However, the vector space is continuous unlike the vocabularies where the representation is discrete. So in this case, what you want to do is to\n",
    "discretize the 128-d space by some clustering method, e.g, KMeans, (you need to choose num_clusters yourself). After you perform clustering using all your training data of size(num_images x num_keypoints, 128), each of these vector will have a cluster label ranging from 0 - num_clusters - 1, then you can treat them as so-called 'visual word'. For each image, now your representation becomes a bag of visual words, quite similar to the representation of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd6def",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Steps to turn SIFT features into trainable features\n",
    "\n",
    "## Gather training descriptors\n",
    "\n",
    "## Perform clustering\n",
    "\n",
    "## Assign Label to train data,validation & test data \n",
    "\n",
    "## Bag of visual words representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e2d38",
   "metadata": {},
   "source": [
    "## Fit into your ML models\n",
    "Try a variable list of classical ML models you've learned so far in class to perform this multi-classification, i.e., \n",
    "RandomForest, Multinomial Logistic Regression(LR for multi-class scenario), and also some others classical approach\n",
    "like SVM(kernel), NaiveBayes etc. Compare the models you've tried on the classification accuracy on test data, plot the confusion matrix. You can experiment on a large vareity of image features not limited to SIFT introduced here, but SIFT can be a decent baseline for you to work on. In this assignment, we do not expect you to be able to achieve high accuracy on the test result since image classification using traditional feature extraction is quited limited. When we later introduce Deep Neural Nets and especially CNN(Convolutional Neural Network), you will easily achieve a high performance. The purpose of this assignment is let you understand and explore how you can do feature engineering in image domain and let you learn some fundamentals of digital image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2be102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

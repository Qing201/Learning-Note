{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Filtering\n",
    "In this programming assignment, we will be looking at Spam Filtering with a real data set that has a \"label\" for every email - i.e. spam or not spam. We will use logistic regression classifier to solve this assignment and participate in a friendly competition on Kaggle (Details below). The assignment goes from data loading to data inspection to data pre-processing to creating a train/test data set to finally doing machine learning, making predictions and evaluating it. This is typically one part of the \"full pipeline\" in ML modeling/prototyping - So you will get a sampler taste of some \"prototype pipeline\" work that happens in practice! Have fun!! And if you get stuck somewhere - Use discord - Maybe someone has a suggestion that will unblock you.\n",
    "\n",
    "The submission consists of two parts:\n",
    "a) A submission of your complete working code with train/validation data sets + your write-up with insights and your learnings (details on this provided below)\n",
    "b) Evaluation of your best model on the Kaggle evaluation data set - For this you can form a team of 2 - To brainstorm ideas and make your best submission. Include your team name, team members in your submission.\n",
    "\n",
    "Kaggle Starting Point for the competition: https://www.kaggle.com/t/7d2850f5b99a41fba457f2ad7acd0fca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team\n",
    "**Team Name on Kaggel:** Qingchuan & Zihe\n",
    "\n",
    "**Team Members:** Qingchuan Hou, Zihe Song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qingchuanhou/opt/anaconda3/envs/Master/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3457: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "all_maills = pd.read_csv(\"email_data/all_emails.csv\",sep=',',index_col=None,engine='python',error_bad_lines=False)\n",
    "test_maills = pd.read_csv(\"email_data/eval_students_2.csv\",sep=',',index_col=None,engine='python',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4260, 3) (1468, 2)\n"
     ]
    }
   ],
   "source": [
    "print(all_maills.shape,test_maills.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5074</td>\n",
       "      <td>Subject: prc review  stinson ,  i am going to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2941</td>\n",
       "      <td>Subject: fwd : hello from charles shen at will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2032</td>\n",
       "      <td>Subject: considered unsolicited bulk email fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3443</td>\n",
       "      <td>Subject: re : looking for someone who has expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3063</td>\n",
       "      <td>Subject: re : term project :  brian ,  no prob...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text\n",
       "0  5074  Subject: prc review  stinson ,  i am going to ...\n",
       "1  2941  Subject: fwd : hello from charles shen at will...\n",
       "2  2032  Subject: considered unsolicited bulk email fro...\n",
       "3  3443  Subject: re : looking for someone who has expe...\n",
       "4  3063  Subject: re : term project :  brian ,  no prob..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maills.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Inspecting the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                                               text  spam\n",
      "0  1235  Subject: naturally irresistible your corporate...     1\n",
      "1  1236  Subject: the stock trading gunslinger  fanny i...     1\n",
      "2  1238  Subject: 4 color printing special  request add...     1\n",
      "3  1239  Subject: do not have money , get software cds ...     1\n",
      "4  1240  Subject: great nnews  hello , welcome to medzo...     1\n"
     ]
    }
   ],
   "source": [
    "# 1. Print a few lines (i.e. each line is an email and a label) from the data_set containing spam (use a pandas functionality - e.g. getting the top lines)\n",
    "\n",
    "print(all_maills.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text  spam\n",
      "1026  2603  Subject: hello guys ,  i ' m \" bugging you \" f...     0\n",
      "1027  2604  Subject: sacramento weather station  fyi  - - ...     0\n",
      "1028  2605  Subject: from the enron india newsdesk - jan 1...     0\n",
      "1029  2606  Subject: re : powerisk 2001 - your invitation ...     0\n",
      "1030  2607  Subject: re : resco database and customer capt...     0\n"
     ]
    }
   ],
   "source": [
    "# 2. Print a few lines from data_set that are not spam\n",
    "\n",
    "print(all_maills[all_maills[\"spam\"] == 0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text  spam\n",
      "4000  6587  Subject: re : var calibration issues  we are p...     0\n",
      "4001  6588  Subject: re : carnegie mellon team meeting  un...     0\n",
      "4002  6590  Subject: re : statistica & lunch  rick ,  we a...     0\n",
      "4003  6592  Subject: re : interview with research dept . c...     0\n",
      "4004  6593  Subject: re : natural gas storage item  vince ...     0\n",
      "4005  6594  Subject: revised : organizational changes  to ...     0\n",
      "4006  6595  Subject: valuation methodology  we ' ve had a ...     0\n",
      "4007  6596  Subject: again , i should have also sent the f...     0\n",
      "4008  6597  Subject: registration materials for nfcf  to :...     0\n",
      "4009  6599  Subject: latest  vince ,  i appologize for shi...     0\n",
      "4010  6601  Subject: visit to enron  grant and vince ,  th...     0\n"
     ]
    }
   ],
   "source": [
    "# 3. Print the emails between lines 5000 and 5010 in the data set\n",
    "print(all_maills.iloc[4000:4011])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data processing step for this HW: \n",
    "Do the following process for all emails in your data set - 1) Tokenize into words 2) Remove stop/filler words and 3) Remove punctuations \n",
    "Below - We have it done for a sample sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "Apply a tokenizer to tokenize the sentences in your email - So your sentence gets broken down to words. We will use a tokenizer from the NLTK library (Natural Language Tool Kit) below for a single sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(x):\n",
    "    y = x.apply(lambda row: word_tokenize(row['text']), axis=1) \n",
    "    return y\n",
    "\n",
    "all_tokenized_text = tokenizer(all_maills)\n",
    "test_tokenized_text = tokenizer(test_maills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_maills['tokenized_text'] = all_maills['text'].apply(word_tokenize)\n",
    "# test_maills['tokenized_text'] = test_maills['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Subject, :, naturally, irresistible, your, co...\n",
       "1    [Subject, :, the, stock, trading, gunslinger, ...\n",
       "2    [Subject, :, 4, color, printing, special, requ...\n",
       "3    [Subject, :, do, not, have, money, ,, get, sof...\n",
       "4    [Subject, :, great, nnews, hello, ,, welcome, ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokenized_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words: Remove Stop Words (or Filler words ) using stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterwords(x):\n",
    "    return x.apply(lambda x: [word for word in x if word not in stopwords.words('english')])\n",
    "\n",
    "all_filtered_text = filterwords(all_tokenized_text)\n",
    "\n",
    "test_filtered_text = filterwords(test_tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [Subject, :, naturally, irresistible, corporat...\n",
      "1    [Subject, :, stock, trading, gunslinger, fanny...\n",
      "2    [Subject, :, 4, color, printing, special, requ...\n",
      "3    [Subject, :, money, ,, get, software, cds, !, ...\n",
      "4    [Subject, :, great, nnews, hello, ,, welcome, ...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(all_filtered_text.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuations: Remove punctuations and other special characters from tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(x):\n",
    "    return x.apply(lambda x: [word for word in x if word.isalnum()] )\n",
    "\n",
    "all_new_text = remove_punctuation(all_filtered_text)\n",
    "test_new_text = remove_punctuation(test_filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [Subject, naturally, irresistible, corporate, ...\n",
      "dtype: object\n",
      "['Subject', 'stock', 'trading', 'gunslinger', 'fanny', 'merrill', 'muzo', 'colza', 'attainder', 'penultimate', 'like', 'esmark', 'perspicuous', 'ramble', 'segovia', 'group', 'try', 'slung', 'kansas', 'tanzania', 'yes', 'chameleon', 'continuant', 'clothesman', 'libretto', 'chesapeake', 'tight', 'waterway', 'herald', 'hawthorn', 'like', 'chisel', 'morristown', 'superior', 'deoxyribonucleic', 'clockwork', 'try', 'hall', 'incredible', 'mcdougall', 'yes', 'hepburn', 'einsteinian', 'earmark', 'sapling', 'boar', 'duane', 'plain', 'palfrey', 'inflexible', 'like', 'huzzah', 'pepperoni', 'bedtime', 'nameable', 'attire', 'try', 'edt', 'chronography', 'optima', 'yes', 'pirogue', 'diffusion', 'albeit']\n"
     ]
    }
   ],
   "source": [
    "print(all_new_text.head(1))\n",
    "print(all_new_text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Exercise: \n",
    "Inspect the resulting list below for any of your emails - Does it look clean and ready to be used for the next step in spam detection? Any other pre-processing steps you can think of or may want to do before spam detection? How about including other NLP features like bi-grams and tri-grams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The another processing I add here is using the lowercase word. And I also have tried the n-grams function in next section when I develop the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(x):\n",
    "    return [[word.lower() for word in maill] for maill in x]\n",
    "\n",
    "all_new_text_lower = lower(all_new_text)\n",
    "test_new_text_lower = lower(test_new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject', 'stock', 'trading', 'gunslinger', 'fanny', 'merrill', 'muzo', 'colza', 'attainder', 'penultimate', 'like', 'esmark', 'perspicuous', 'ramble', 'segovia', 'group', 'try', 'slung', 'kansas', 'tanzania', 'yes', 'chameleon', 'continuant', 'clothesman', 'libretto', 'chesapeake', 'tight', 'waterway', 'herald', 'hawthorn', 'like', 'chisel', 'morristown', 'superior', 'deoxyribonucleic', 'clockwork', 'try', 'hall', 'incredible', 'mcdougall', 'yes', 'hepburn', 'einsteinian', 'earmark', 'sapling', 'boar', 'duane', 'plain', 'palfrey', 'inflexible', 'like', 'huzzah', 'pepperoni', 'bedtime', 'nameable', 'attire', 'try', 'edt', 'chronography', 'optima', 'yes', 'pirogue', 'diffusion', 'albeit']\n"
     ]
    }
   ],
   "source": [
    "print(all_new_text_lower[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_new = all_maills.copy()\n",
    "test_new = test_maills.copy()\n",
    "\n",
    "all_new['text'] = [' '.join(map(str, l)) for l in all_new_text_lower]  # change list to string\n",
    "test_new['text'] = [' '.join(map(str, l)) for l in test_new_text_lower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                                               text  spam\n",
      "0  1235  subject naturally irresistible corporate ident...     1\n",
      "1  1236  subject stock trading gunslinger fanny merrill...     1\n",
      "2  1238  subject 4 color printing special request addit...     1\n",
      "3  1239  subject money get software cds software compat...     1\n",
      "4  1240  subject great nnews hello welcome medzonline s...     1\n",
      "     id                                               text\n",
      "0  5074  subject prc review stinson going prc review bo...\n",
      "1  2941  subject fwd hello charles shen williams co ton...\n",
      "2  2032  subject considered unsolicited bulk email mess...\n",
      "3  3443  subject looking someone experience finance mat...\n",
      "4  3063  subject term project brian problem vince brian...\n"
     ]
    }
   ],
   "source": [
    "print(all_new.head())\n",
    "print(test_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject stock trading gunslinger fanny merrill muzo colza attainder penultimate like esmark perspicuous ramble segovia group try slung kansas tanzania yes chameleon continuant clothesman libretto chesapeake tight waterway herald hawthorn like chisel morristown superior deoxyribonucleic clockwork try hall incredible mcdougall yes hepburn einsteinian earmark sapling boar duane plain palfrey inflexible like huzzah pepperoni bedtime nameable attire try edt chronography optima yes pirogue diffusion albeit\n"
     ]
    }
   ],
   "source": [
    "print(all_new['text'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Train/Validation Split\n",
    "Now for each email in your data set - You have boiled the email down to its essentials - A list of words that are clean and ready for some Machine Learning! Maybe punctuations matter for spam emails!!? \n",
    "If you wish to keep them, you may for your curiosity and see how it impacts metrics (i.e. skip step 3 above). \n",
    "\n",
    "What we will do now is split the data set into train and test set - The train set can have 80% of the data (i.e. emails along with their labels) chosen at random - But with good representation from both spam and not-spam email classes. And the same goes for the test set - Which would have the remaining 20% of the data.\n",
    "Look up python libraries that can do this data split for you automatically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_new.groupby(\"spam\").sample(frac=0.8)\n",
    "valid = all_new[~all_new.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data shape: (4260, 3)\n",
      "Train data shape: (3408, 3)\n",
      "Validation data shape: (852, 3)\n"
     ]
    }
   ],
   "source": [
    "print('All data shape:', all_new.shape)\n",
    "print('Train data shape:', train.shape)\n",
    "print('Validation data shape:', valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = train['text'], train['spam']\n",
    "valid_X, valid_y = valid['text'], valid['spam']\n",
    "test_X = test_new['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Train your model and evaluate on Kaggle\n",
    "Report your train/validation F1-score for your baseline model (starter LR model) and also your best LR model. Also report your insights on what worked and what did not on the Kaggle evaluation. How can your model be improved? Where does your model make mistakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('tf-idf',  TfidfVectorizer(max_df=0.6, min_df=1, sublinear_tf=True,ngram_range=(1,1))),\n",
    "                    ('LR', LogisticRegression()),\n",
    "                     ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LogisticRegression())])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.9753521126760564\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(valid_X)\n",
    "print('accuracy_score:',accuracy_score(valid_y,predicted))\n",
    "print('f-1 score:', f1_score(valid_y,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.9906103286384976\n",
      "f-1 score: 0.98\n"
     ]
    }
   ],
   "source": [
    "# I put the sample data code here again so I can only run one block here when I test my model with different data\n",
    "train = all_new.groupby(\"spam\").sample(frac=0.95)\n",
    "valid = all_new[~all_new.index.isin(train.index)]\n",
    "train_X, train_y = train['text'], train['spam']\n",
    "valid_X, valid_y = valid['text'], valid['spam']\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "predicted = model.predict(valid_X)\n",
    "print('accuracy_score',accuracy_score(valid_y,predicted))\n",
    "print('f-1 score:', f1_score(valid_y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new['spam'] = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text  spam\n",
      "0     5074  subject prc review stinson going prc review bo...     0\n",
      "1     2941  subject fwd hello charles shen williams co ton...     0\n",
      "2     2032  subject considered unsolicited bulk email mess...     1\n",
      "3     3443  subject looking someone experience finance mat...     0\n",
      "4     3063  subject term project brian problem vince brian...     0\n",
      "...    ...                                                ...   ...\n",
      "1463  6232  subject job application enron research group d...     0\n",
      "1464  3227  subject lawyer ian sorry delay responding curr...     0\n",
      "1465  5773  subject enron india mark two points 1 probably...     0\n",
      "1466  5421  subject sorry see 11 30 hyatt lobby vince j ka...     0\n",
      "1467  4708  subject departure research group unique extrao...     0\n",
      "\n",
      "[1468 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the predated data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id  spam\n",
      "0  5074     0\n",
      "1  2941     0\n",
      "2  2032     1\n",
      "3  3443     0\n",
      "4  3063     0\n",
      "(1468, 2)\n"
     ]
    }
   ],
   "source": [
    "prediction = test_new[['id','spam']]\n",
    "\n",
    "print(prediction.head())\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_csv('prediction.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we build got a 0.97955 F1 score. The F1 score of our final model on Kaggle is 0.98722. It increased by 0.008.\n",
    "\n",
    "For the first model, I only did the instruction step on data processing. Then use the basic tfidf and LR model.\n",
    "\n",
    "I did a few improvements between the first model and the last one. First, I added the lowercase function on the data processing step. Second, I used the `max_df` function for tf-idf to make my model only focus on the more important words. These two function both gived us a better accuracy of our model.\n",
    "\n",
    "I also tried using the n-grams function, but it seems could not improve my model. Also, any other change in the data processing will decrease the score of my model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
